{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXl5PzoKDEmK"
   },
   "source": [
    "<h1 align=\"center\">Deep Learning - Master in Deep Learning of UPM</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3uHCp0VDEmM"
   },
   "source": [
    "**IMPORTANTE**\n",
    "\n",
    "Antes de empezar debemos instalar PyTorch Lightning, por defecto, esto valdría:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SArzmQSlDEmO"
   },
   "outputs": [],
   "source": [
    "!pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O23KCJuqDEmQ"
   },
   "source": [
    "Además, si te encuentras ejecutando este código en Google Collab, lo mejor será que montes tu drive para tener acceso a los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OCtv-uFjDEmQ"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C73taijTDEmT"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "DATA_PATH = 'data/stocks.csv'\n",
    "SEED = 42\n",
    "seed_everything(seed=SEED) # Fijamos una semilla para reproducibilidad en los experimentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZ2nhhgyDEmU"
   },
   "source": [
    "\n",
    "\n",
    "Este dataset consta del precio de cierre de la acción de Amazon (AMZN) desde 2006 hasta 2017. Es el mismo que utilizamos durante la practica de redes recurrentes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pWbsdELvDEmW"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(DATA_PATH)\n",
    "\n",
    "data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')\n",
    "data.sort_values('date', inplace=True)\n",
    "print(f\"Date range: {data['date'].min()} to {data['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qPHAM0sXoKYz"
   },
   "source": [
    "Esta es la distribución de los datos, también usaremos un escalador para evitar los problemas de magnitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7d0GHTb7ntMk"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(DATA_PATH)\n",
    "data.plot(x='date', y='close', title='AMZN stock price', ylabel='Price', xlabel='Date', figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AScwtwcZDEmX"
   },
   "source": [
    "\n",
    "\n",
    "Como esto ya lo hemos hecho en un punto anterior del tiempo, lo tendremos disponible! Dataset, DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d8-cyhWhoVy4"
   },
   "outputs": [],
   "source": [
    "class StocksDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, w=10, h=1):\n",
    "        self.data = df.drop('date', axis=1).values\n",
    "        self.w = w\n",
    "        self.h = h\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - (self.w + self.h) + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = self.data[idx:idx+self.w] # [i: i+w)\n",
    "        target = self.data[idx+self.w: idx+self.w+self.h].reshape(-1) # [i+w, i+w+h)\n",
    "        return features, target # (w, input_size), (h,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f9zwv4EUDEmX"
   },
   "outputs": [],
   "source": [
    "class StocksDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, df, w=10, h=1, batch_size=16, val_size=0.2, test_size=0.2):\n",
    "        super().__init__()\n",
    "        self.data = df\n",
    "\n",
    "        self.sequential_train_val_test_split(df, val_size=val_size, test_size=test_size)\n",
    "        self.normalize()\n",
    "\n",
    "        self.w = w\n",
    "        self.h = h\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == 'fit':\n",
    "            self.train_dataset = StocksDataset(self.train_df, w=self.w, h=self.h)\n",
    "            self.val_dataset = StocksDataset(self.val_df, w=self.w, h=self.h)\n",
    "        elif stage == 'test':\n",
    "            self.test_dataset = StocksDataset(self.test_df, w=self.w, h=self.h)\n",
    "\n",
    "    def normalize(self):\n",
    "        self.scaler_train = MinMaxScaler()\n",
    "        self.scaler_val = MinMaxScaler()\n",
    "        self.scaler_test = MinMaxScaler()\n",
    "\n",
    "        # Ajusta y transforma cada split\n",
    "        self.train_df['close'] = self.scaler_train.fit_transform(self.train_df[['close']])\n",
    "        self.val_df['close'] = self.scaler_val.fit_transform(self.val_df[['close']])\n",
    "        self.test_df['close'] = self.scaler_test.fit_transform(self.test_df[['close']])\n",
    "\n",
    "    def sequential_train_val_test_split(self, df, val_size=0.2, test_size=0.2):\n",
    "        # Aseguramos el formato de la fecha y ordenamos por ella\n",
    "        df['date'] = pd.to_datetime(df['date'], format='%d/%m/%Y')\n",
    "        df.sort_values('date', inplace=True)\n",
    "\n",
    "        # Calculamos los índices para hacer los splits\n",
    "        n = len(df)\n",
    "        train_end = int((1 - val_size - test_size) * n)\n",
    "        val_end = int((1 - test_size) * n)\n",
    "\n",
    "        self.train_df = df.iloc[:train_end].copy()\n",
    "        self.val_df = df.iloc[train_end:val_end].copy()\n",
    "        self.test_df = df.iloc[val_end:].copy()\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        features, targets = zip(*batch)\n",
    "\n",
    "        features = np.stack(features, axis=0)  # [batch_size, w, input_size]\n",
    "        targets = np.stack(targets, axis=0)    # [batch_size, h, input_size]\n",
    "\n",
    "        features = torch.tensor(features, dtype=torch.float32)\n",
    "        targets = torch.tensor(targets, dtype=torch.float32)\n",
    "        return features, targets\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, collate_fn=self.collate_fn)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, collate_fn=self.collate_fn)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, collate_fn=self.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1cIf3GkkMrFI"
   },
   "source": [
    "Vamos a crear nuestro propios módulos de atención. Todos ellos queremos que sean entrenables! Así que los diseñaremos con pesos.\n",
    "Hemos aprendido como calcular la LSTM con pesos entrenables de atención, pero no como ocurre en otros módulos de atención.\n",
    "Para ello tendremos que hacer una llamada previa a un capa tipo `linear(q)`, `linear(k)`, `linear(v)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pvwf4dwjOc23"
   },
   "outputs": [],
   "source": [
    "class TrainableAdditiveAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Modulo de atencion\n",
    "    hidden_dim[int]: tamaño de la representación\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(TrainableAdditiveAttention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.W1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.W2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.W3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.V = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def _score(self, q, k):\n",
    "        return self.V(torch.tanh(self.W1(q) + self.W2(k)))\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        score = self._score(q, k) # Q[batch_size; seq_len, hidden_dim]\n",
    "        attention_weights = torch.softmax(score, dim=1)\n",
    "        context_vector = attention_weights * self.W3(v) # Una atencion para cada elem. de la secuencia\n",
    "        return context_vector, attention_weights #C[batch_size; seq_len; hidden_dim] // #A[batch_size; seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bMaobx_4sOUd"
   },
   "outputs": [],
   "source": [
    "class TrainableGeneralAttention(nn.Module):\n",
    "    def __init__(self, query_dim, key_dim, value_dim):\n",
    "        super(TrainableGeneralAttention, self).__init__()\n",
    "        self.W_a = nn.Parameter(torch.randn(query_dim, key_dim))\n",
    "        self.Wq = nn.Linear(query_dim, query_dim)\n",
    "        self.Wk = nn.Linear(key_dim, key_dim)\n",
    "        self.Wv = nn.Linear(value_dim, value_dim)\n",
    "\n",
    "    def _score(self, q, k):\n",
    "        # Q = Query, K = Key\n",
    "        # Las dimensiones de Q y K tienen que ser compatibles!\n",
    "        # Mecanismo de la atencion general\n",
    "        left = torch.matmul(q, self.W_a)\n",
    "        return torch.matmul(left, k.transpose(-2, -1))\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        # Q = Query, K = Key, V = Value\n",
    "        # Las dimensiones de Q, K y V tienen que ser compatibles!\n",
    "        score = self._score(q, k) #C[batch_size; seq_len; hidden_dim]\n",
    "        attention_weights = torch.softmax(score, dim=1)\n",
    "        # Cuidado, esto es el producto matricial!\n",
    "        context_vector = torch.matmul(attention_weights, v)\n",
    "        return context_vector, attention_weights #C[batch_size; seq_len; hidden_dim] // #A[batch_size; seq_len; seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "trifMTSVsRXN"
   },
   "outputs": [],
   "source": [
    "class TrainableScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, query_dim, key_dim, value_dim):\n",
    "        super(TrainableScaledDotProductAttention, self).__init__()\n",
    "        self.Wk = nn.Linear(key_dim, key_dim)\n",
    "        self.Wq = nn.Linear(query_dim, query_dim)\n",
    "        self.Wv = nn.Linear(value_dim, value_dim)\n",
    "\n",
    "    def _score(self, q, k):\n",
    "        # Q = Query, K = Key\n",
    "        # Las dimensiones de Q y K tienen que ser compatibles!\n",
    "        # Mecanismo de la atencion general\n",
    "        return torch.matmul(q, k.transpose(-2, -1)) / np.sqrt(q.size(-1))\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        # Q = Query, K = Key, V = Value\n",
    "        # Las dimensiones de Q, K y V tienen que ser compatibles!\n",
    "        score = self._score(self.Wq(q), self.Wk(k)) #C[batch_size; seq_len; hidden_dim]\n",
    "        attention_weights = torch.softmax(score, dim=1)\n",
    "        # Cuidado, esto es el producto matricial!\n",
    "        context_vector = torch.matmul(attention_weights, self.Wv(v))\n",
    "        return context_vector, attention_weights #C[batch_size; seq_len; hidden_dim] // #A[batch_size; seq_len; seq_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oR4xBVwxDEmY"
   },
   "source": [
    "El siguiente paso será definir la LSTM compatible con atencion y con multiples métodos de pooling. Vamos a crear un nuevo módulo que incorpora las atenciones propuestas. También vamos a generalizar la definición de la LSTM para poder intercalar sucesivas atenciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TsSGyX4SDEmY"
   },
   "outputs": [],
   "source": [
    "class AdvancedAttentionLSTMRegressor(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM Regressor model\n",
    "    h[int]: horizonte de predicción\n",
    "    input_size[int]: variables de la serie temporal\n",
    "    hidden_size[int]: tamaño de las capas ocultas de la RNN\n",
    "    num_layers[int]: número de capas de la RNN (si > 1, stacking de células RNN)\n",
    "    batch_first[bool]: si el batch_size es la primera dimensión\n",
    "    p_drop[float]: probabilidad de dropout\n",
    "    \"\"\"\n",
    "    def __init__(self,  h=1,\n",
    "                 input_size=1,\n",
    "                 hidden_size=64,\n",
    "                 num_layers=1,\n",
    "                 batch_first=True,\n",
    "                 p_drop=0.0,\n",
    "                 attention_type='None',\n",
    "                 pooling_type='last'):\n",
    "        super(AdvancedAttentionLSTMRegressor, self).__init__()\n",
    "        self.lstm_init_layer = nn.LSTM(input_size=input_size,\n",
    "                                       hidden_size=hidden_size,\n",
    "                                       num_layers=1,\n",
    "                                       batch_first=batch_first)\n",
    "        self.lstm_layers = nn.ModuleList([nn.LSTM(input_size=hidden_size,\n",
    "                                                  hidden_size=hidden_size,\n",
    "                                                  num_layers=1,\n",
    "                                                  batch_first=batch_first,\n",
    "                                                  ) for i in range(num_layers-1)])\n",
    "        self.dropout_layers = nn.ModuleList([nn.Dropout(p_drop) for i in range(num_layers-1)]) # No hay dropout en la ultima capa!\n",
    "        self.pooling_type = pooling_type\n",
    "        self.attention_type = None\n",
    "        # Attention types\n",
    "        if attention_type == 'additive':\n",
    "          self.attention_type = TrainableAdditiveAttention\n",
    "          self.attention_layers = nn.ModuleList([self.attention_type(hidden_size) for i in range(num_layers)])\n",
    "        elif attention_type == 'general':\n",
    "          self.attention_type = TrainableGeneralAttention\n",
    "          self.attention_layers = nn.ModuleList([self.attention_type(hidden_size, hidden_size, hidden_size) for i in range(num_layers)])\n",
    "        elif attention_type == 'sdpa':\n",
    "          self.attention_type = TrainableScaledDotProductAttention\n",
    "          self.attention_layers = nn.ModuleList([self.attention_type(hidden_size, hidden_size, hidden_size) for i in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, h)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm_init_layer(x)\n",
    "        \n",
    "        for i, lstm_layer in enumerate(self.lstm_layers):\n",
    "          x = self.dropout_layers[i](x)\n",
    "          if self.attention_type:\n",
    "            x, _ = self.attention_layers[i](x,x,x)\n",
    "          x, _ = lstm_layer(x)\n",
    "\n",
    "        if self.attention_type:\n",
    "          x, _ = self.attention_layers[-1](x,x,x)\n",
    "\n",
    "        if self.pooling_type == 'last':\n",
    "          x = x[:, -1, :]\n",
    "        elif self.pooling_type == 'mean':\n",
    "          x = x.mean(dim=1)\n",
    "        elif self.pooling_type == 'max':\n",
    "          x = x.max(dim=1)[0]\n",
    "\n",
    "        return self.fc(x) #out[batch_size; h]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-1ixBMEDEmZ"
   },
   "source": [
    "Declaramos el Lighting Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MN2OXyyiDEmZ"
   },
   "outputs": [],
   "source": [
    "class StockPredictor(pl.LightningModule):\n",
    "    def __init__(self, model, learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters() # guardamos la configuración de hiperparámetros\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = model\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def compute_batch(self, batch, split='train'):\n",
    "        inputs, targets = batch\n",
    "        output = self(inputs)\n",
    "\n",
    "        preds = output.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        loss = self.criterion(preds, targets)\n",
    "        self.log_dict(\n",
    "            {\n",
    "                f'{split}_loss': loss,\n",
    "            },\n",
    "            on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.compute_batch(batch, 'train')\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.compute_batch(batch, 'val')\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.compute_batch(batch, 'test')\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.learning_rate) # self.parameters() son los parámetros del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DNigCJQ18ae0"
   },
   "source": [
    "Explora varios parametros y configuraciones, observa que ocurre al entrenamiento con cada mecanismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dH6IrG2y7yNU"
   },
   "outputs": [],
   "source": [
    "# @title Seleccion parametros\n",
    "w = 10 #@param {type:\"integer\"}\n",
    "h = 3 #@param {type:\"integer\"}\n",
    "input_size = 1 #@param {type:\"integer\"}\n",
    "batch_size = 64 #@param {type:\"integer\"}\n",
    "num_layers = 1 #@param {type:\"integer\"}\n",
    "hidden_size = 128 #@param {type:\"integer\"}\n",
    "learning_rate = 1e-3 #@param {type:\"number\"}\n",
    "p_drop = 0.2 #@param {type:\"number\"}\n",
    "pooling = 'last' #@param [\"last\", \"mean\", \"max\"]\n",
    "attention = 'none' #@param [\"none\", \"additive\", \"general\", \"sdpa\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uvpogzW7xfbc"
   },
   "outputs": [],
   "source": [
    "# Parámetros\n",
    "SAVE_DIR = f'lightning_logs/stock/{datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}'\n",
    "\n",
    "# DataModule\n",
    "data = pd.read_csv(DATA_PATH)\n",
    "data_module = StocksDataModule(data, w=w, h=h, batch_size=batch_size)\n",
    "\n",
    "# Model\n",
    "model = AdvancedAttentionLSTMRegressor(h=h, input_size=input_size,\n",
    "                                       hidden_size=hidden_size,\n",
    "                                       num_layers=num_layers,\n",
    "                                       batch_first=True,\n",
    "                                       p_drop=p_drop,\n",
    "                                       pooling=pooling,\n",
    "                                       attention=attention)\n",
    "\n",
    "# LightningModule\n",
    "module = StockPredictor(model, learning_rate=learning_rate)\n",
    "\n",
    "# Callbacks\n",
    "early_stopping_callback = pl.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', # monitorizamos la pérdida en el conjunto de validación\n",
    "    mode='min',\n",
    "    patience=5, # número de epochs sin mejora antes de parar\n",
    "    verbose=False, # si queremos que muestre mensajes del estado del early stopping\n",
    ")\n",
    "model_checkpoint_callback = pd.callbacks.ModelCheckpoint(\n",
    "    monitor='val_loss', # monitorizamos la pérdida en el conjunto de validación\n",
    "    mode='min', # queremos minimizar la pérdida\n",
    "    save_top_k=1, # guardamos solo el mejor modelo\n",
    "    dirpath=SAVE_DIR, # directorio donde se guardan los modelos\n",
    "    filename=f'best_model' # nombre del archivo\n",
    ")\n",
    "\n",
    "# Descomentar en función de si queremos o no el callback de forecasting\n",
    "# forecasting_callback = ForecastingCallback()\n",
    "# callbacks = [early_stopping_callback, model_checkpoint_callback, forecasting_callback]\n",
    "\n",
    "callbacks = [early_stopping_callback, model_checkpoint_callback]\n",
    "\n",
    "# Loggers\n",
    "csv_logger = pl.loggers.CSVLogger(\n",
    "    save_dir=SAVE_DIR,\n",
    "    name='metrics',\n",
    "    version=None\n",
    ")\n",
    "\n",
    "loggers = [csv_logger] # se pueden poner varios loggers (mirar documentación)\n",
    "\n",
    "# Trainer\n",
    "trainer = pl.Trainer(max_epochs=50, accelerator='gpu', devices=[0], callbacks=callbacks, logger=loggers)\n",
    "\n",
    "trainer.fit(module, data_module)\n",
    "results = trainer.test(module, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h880g3GMxkGE"
   },
   "outputs": [],
   "source": [
    "# Parámetros\n",
    "SAVE_DIR = f'lightning_logs/stock/{datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}'\n",
    "\n",
    "# DataModule\n",
    "data = pd.read_csv(DATA_PATH)\n",
    "data_module = StocksDataModule(data, w=w, h=h, batch_size=batch_size)\n",
    "\n",
    "# Model\n",
    "model = AdvancedAttentionLSTMRegressor(h=h, input_size=input_size,\n",
    "                                       hidden_size=hidden_size,\n",
    "                                       num_layers=num_layers,\n",
    "                                       batch_first=True,\n",
    "                                       p_drop=p_drop,\n",
    "                                       pooling_type=pooling,\n",
    "                                       attention_type=attention)\n",
    "\n",
    "# LightningModule\n",
    "module = StockPredictor(model, learning_rate=learning_rate)\n",
    "\n",
    "# Callbacks\n",
    "early_stopping_callback = pl.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', # monitorizamos la pérdida en el conjunto de validación\n",
    "    mode='min',\n",
    "    patience=5, # número de epochs sin mejora antes de parar\n",
    "    verbose=False, # si queremos que muestre mensajes del estado del early stopping\n",
    ")\n",
    "model_checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    monitor='val_loss', # monitorizamos la pérdida en el conjunto de validación\n",
    "    mode='min', # queremos minimizar la pérdida\n",
    "    save_top_k=1, # guardamos solo el mejor modelo\n",
    "    dirpath=SAVE_DIR, # directorio donde se guardan los modelos\n",
    "    filename=f'best_model' # nombre del archivo\n",
    ")\n",
    "\n",
    "# Descomentar en función de si queremos o no el callback de forecasting\n",
    "# forecasting_callback = ForecastingCallback()\n",
    "# callbacks = [early_stopping_callback, model_checkpoint_callback, forecasting_callback]\n",
    "\n",
    "callbacks = [early_stopping_callback, model_checkpoint_callback]\n",
    "\n",
    "# Loggers\n",
    "csv_logger = pl.loggers.CSVLogger(\n",
    "    save_dir=SAVE_DIR,\n",
    "    name='metrics',\n",
    "    version=None\n",
    ")\n",
    "\n",
    "loggers = [csv_logger] # se pueden poner varios loggers (mirar documentación)\n",
    "\n",
    "# Trainer\n",
    "trainer = pl.Trainer(max_epochs=50, accelerator='cpu', callbacks=callbacks, logger=loggers)\n",
    "\n",
    "trainer.fit(module, data_module)\n",
    "results = trainer.test(module, data_module)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
