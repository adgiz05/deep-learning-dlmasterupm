{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXl5PzoKDEmK"
   },
   "source": [
    "<h1 align=\"center\">Deep Learning - Master in Deep Learning of UPM</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3uHCp0VDEmM"
   },
   "source": [
    "**IMPORTANTE**\n",
    "\n",
    "Antes de empezar debemos instalar PyTorch Lightning, por defecto, esto valdría:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6233,
     "status": "ok",
     "timestamp": 1733832382456,
     "user": {
      "displayName": "Javier Huertas",
      "userId": "08531127118556167809"
     },
     "user_tz": -60
    },
    "id": "SArzmQSlDEmO",
    "outputId": "d1dd6ebe-5dac-413b-aaeb-e0d9eb8b1ce7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-lightning\n",
      "  Downloading pytorch_lightning-2.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.5.1+cu121)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.6)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.2)\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2024.10.0)\n",
      "Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n",
      "  Downloading torchmetrics-1.6.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.12.2)\n",
      "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning)\n",
      "  Downloading lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.11.9)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (75.1.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.16.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.1.0->pytorch-lightning) (1.3.0)\n",
      "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics>=0.7.0->pytorch-lightning) (1.26.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.18.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (3.0.2)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.10)\n",
      "Downloading pytorch_lightning-2.4.0-py3-none-any.whl (815 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n",
      "Downloading torchmetrics-1.6.0-py3-none-any.whl (926 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m926.4/926.4 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lightning-utilities, torchmetrics, pytorch-lightning\n",
      "Successfully installed lightning-utilities-0.11.9 pytorch-lightning-2.4.0 torchmetrics-1.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O23KCJuqDEmQ"
   },
   "source": [
    "Además, si te encuentras ejecutando este código en Google Collab, lo mejor será que montes tu drive para tener acceso a los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19464,
     "status": "ok",
     "timestamp": 1733832401915,
     "user": {
      "displayName": "Javier Huertas",
      "userId": "08531127118556167809"
     },
     "user_tz": -60
    },
    "id": "OCtv-uFjDEmQ",
    "outputId": "eda2fd79-c653-4ee7-c092-71c6fea9917c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37180,
     "status": "ok",
     "timestamp": 1733832439086,
     "user": {
      "displayName": "Javier Huertas",
      "userId": "08531127118556167809"
     },
     "user_tz": -60
    },
    "id": "C73taijTDEmT",
    "outputId": "e3085379-8e65-4da0-8caf-dea12790ba12"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightning_fabric.utilities.seed:Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "DATA_PATH = 'data/stocks.csv'\n",
    "SEED = 42\n",
    "seed_everything(seed=SEED) # Fijamos una semilla para reproducibilidad en los experimentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZ2nhhgyDEmU"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "Este dataset consta del precio de cierre de la acción de Amazon (AMZN) desde 2006 hasta 2017. Es el mismo que utilizamos durante la practica de redes recurrentes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 203,
     "status": "ok",
     "timestamp": 1733832447351,
     "user": {
      "displayName": "Javier Huertas",
      "userId": "08531127118556167809"
     },
     "user_tz": -60
    },
    "id": "pWbsdELvDEmW",
    "outputId": "25309c03-2a13-4351-91e2-0a01dbf54c7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date range: 2010-01-01 00:00:00 to 2019-12-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(DATA_PATH)\n",
    "\n",
    "data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')\n",
    "data.sort_values('date', inplace=True)\n",
    "print(f\"Date range: {data['date'].min()} to {data['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qPHAM0sXoKYz"
   },
   "source": [
    "Esta es la distribución de los datos, también usaremos un escalador para evitar los problemas de magnitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7d0GHTb7ntMk"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(DATA_PATH)\n",
    "data.plot(x='date', y='close', title='AMZN stock price', ylabel='Price', xlabel='Date', figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AScwtwcZDEmX"
   },
   "source": [
    "\n",
    "\n",
    "Como esto ya lo hemos hecho en un punto anterior del tiempo, lo tendremos disponible! Dataset, DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d8-cyhWhoVy4"
   },
   "outputs": [],
   "source": [
    "class StocksDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, w=10, h=1):\n",
    "        self.data = df.drop('date', axis=1).values\n",
    "        self.w = w\n",
    "        self.h = h\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - (self.w + self.h) + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = self.data[idx:idx+self.w] # [i: i+w)\n",
    "        target = self.data[idx+self.w: idx+self.w+self.h].reshape(-1) # [i+w, i+w+h)\n",
    "        return features, target # (w, input_size), (h,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f9zwv4EUDEmX"
   },
   "outputs": [],
   "source": [
    "class StocksDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, df, w=10, h=1, batch_size=16, val_size=0.2, test_size=0.2):\n",
    "        super().__init__()\n",
    "        self.data = df\n",
    "\n",
    "        self.sequential_train_val_test_split(df, val_size=val_size, test_size=test_size)\n",
    "        self.normalize()\n",
    "\n",
    "        self.w = w\n",
    "        self.h = h\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == 'fit':\n",
    "            self.train_dataset = StocksDataset(self.train_df, w=self.w, h=self.h)\n",
    "            self.val_dataset = StocksDataset(self.val_df, w=self.w, h=self.h)\n",
    "        elif stage == 'test':\n",
    "            self.test_dataset = StocksDataset(self.test_df, w=self.w, h=self.h)\n",
    "\n",
    "    def normalize(self):\n",
    "        self.scaler_train = MinMaxScaler()\n",
    "        self.scaler_val = MinMaxScaler()\n",
    "        self.scaler_test = MinMaxScaler()\n",
    "\n",
    "        # Ajusta y transforma cada split\n",
    "        self.train_df['close'] = self.scaler_train.fit_transform(self.train_df[['close']])\n",
    "        self.val_df['close'] = self.scaler_val.fit_transform(self.val_df[['close']])\n",
    "        self.test_df['close'] = self.scaler_test.fit_transform(self.test_df[['close']])\n",
    "\n",
    "    def sequential_train_val_test_split(self, df, val_size=0.2, test_size=0.2):\n",
    "        # Aseguramos el formato de la fecha y ordenamos por ella\n",
    "        df['date'] = pd.to_datetime(df['date'], format='%d/%m/%Y')\n",
    "        df.sort_values('date', inplace=True)\n",
    "\n",
    "        # Calculamos los índices para hacer los splits\n",
    "        n = len(df)\n",
    "        train_end = int((1 - val_size - test_size) * n)\n",
    "        val_end = int((1 - test_size) * n)\n",
    "\n",
    "        self.train_df = df.iloc[:train_end].copy()\n",
    "        self.val_df = df.iloc[train_end:val_end].copy()\n",
    "        self.test_df = df.iloc[val_end:].copy()\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        features, targets = zip(*batch)\n",
    "\n",
    "        features = np.stack(features, axis=0)  # [batch_size, w, input_size]\n",
    "        targets = np.stack(targets, axis=0)    # [batch_size, h, input_size]\n",
    "\n",
    "        features = torch.tensor(features, dtype=torch.float32)\n",
    "        targets = torch.tensor(targets, dtype=torch.float32)\n",
    "        return features, targets\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, collate_fn=self.collate_fn)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, collate_fn=self.collate_fn)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, collate_fn=self.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1cIf3GkkMrFI"
   },
   "source": [
    "## Modulos de Atencion\n",
    "Vamos a crear nuestro propios módulos de atención. Todos ellos queremos que sean entrenables! Así que los diseñaremos con pesos.\n",
    "Hemos aprendido como calcular la LSTM con pesos entrenables de atención, pero no como ocurre en otros módulos de atención.\n",
    "Para ello tendremos que hacer una llamada previa a un capa tipo `linear(q)`, `linear(k)`, `linear(v)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pvwf4dwjOc23"
   },
   "outputs": [],
   "source": [
    "class TrainableAdditiveAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Modulo de atencion\n",
    "    hidden_dim[int]: tamaño de la representación\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(TrainableAdditiveAttention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.W1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.W2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.W3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.V = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def _score(self, q, k):\n",
    "        ...\n",
    "        pass\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        ...\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bMaobx_4sOUd"
   },
   "outputs": [],
   "source": [
    "class TrainableGeneralAttention(nn.Module):\n",
    "    def __init__(self, query_dim, key_dim, value_dim):\n",
    "        super(TrainableGeneralAttention, self).__init__()\n",
    "        self.W_a = nn.Parameter(torch.randn(query_dim, key_dim))\n",
    "        self.Wq = nn.Linear(query_dim, query_dim)\n",
    "        self.Wk = nn.Linear(key_dim, key_dim)\n",
    "        self.Wv = nn.Linear(value_dim, value_dim)\n",
    "\n",
    "    def _score(self, q, k):\n",
    "        ...\n",
    "        pass\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        ...\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "trifMTSVsRXN"
   },
   "outputs": [],
   "source": [
    "class TrainableScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, query_dim, key_dim, value_dim):\n",
    "        super(TrainableScaledDotProductAttention, self).__init__()\n",
    "        self.Wk = nn.Linear(key_dim, key_dim)\n",
    "        self.Wq = nn.Linear(query_dim, query_dim)\n",
    "        self.Wv = nn.Linear(value_dim, value_dim)\n",
    "\n",
    "    def _score(self, q, k):\n",
    "        ...\n",
    "        pass\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        ...\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oR4xBVwxDEmY"
   },
   "source": [
    "## LSTM Mejorada\n",
    "El siguiente paso será definir la LSTM compatible con atencion y con multiples métodos de pooling. Vamos a crear un nuevo módulo que incorpora las atenciones propuestas. También vamos a generalizar la definición de la LSTM para poder intercalar sucesivas atenciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TsSGyX4SDEmY"
   },
   "outputs": [],
   "source": [
    "class AdvancedAttentionLSTMRegressor(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM Regressor model\n",
    "    h[int]: horizonte de predicción\n",
    "    input_size[int]: variables de la serie temporal\n",
    "    hidden_size[int]: tamaño de las capas ocultas de la RNN\n",
    "    num_layers[int]: número de capas de la RNN (si > 1, stacking de células RNN)\n",
    "    batch_first[bool]: si el batch_size es la primera dimensión\n",
    "    p_drop[float]: probabilidad de dropout\n",
    "    \"\"\"\n",
    "    def __init__(self,  h=1,\n",
    "                 input_size=1,\n",
    "                 hidden_size=64,\n",
    "                 num_layers=1,\n",
    "                 batch_first=True,\n",
    "                 p_drop=0.0,\n",
    "                 attention_type='None',\n",
    "                 pooling_type='last'):\n",
    "        super(AdvancedAttentionLSTMRegressor, self).__init__()\n",
    "        self.lstm_init_layer = nn.LSTM(input_size=input_size,\n",
    "                                       hidden_size=hidden_size,\n",
    "                                       num_layers=1,\n",
    "                                       batch_first=batch_first)\n",
    "        self.lstm_layers = nn.ModuleList([nn.LSTM(input_size=hidden_size,\n",
    "                                                  hidden_size=hidden_size,\n",
    "                                                  num_layers=1,\n",
    "                                                  batch_first=batch_first,\n",
    "                                                  ) for i in range(num_layers-1)])\n",
    "        self.dropout_layers = nn.ModuleList([nn.Dropout(p_drop) for i in range(num_layers-1)]) # No hay dropout en la ultima capa!\n",
    "        self.pooling_type = pooling_type\n",
    "        self.attention_type = None\n",
    "        # Attention types\n",
    "        if attention_type == 'additive':\n",
    "          self.attention_type = TrainableAdditiveAttention\n",
    "          self.attention_layers = nn.ModuleList([self.attention_type(hidden_size) for i in range(num_layers)])\n",
    "        elif attention_type == 'general':\n",
    "          self.attention_type = TrainableGeneralAttention\n",
    "          self.attention_layers = nn.ModuleList([self.attention_type(hidden_size, hidden_size, hidden_size) for i in range(num_layers)])\n",
    "        elif attention_type == 'sdpa':\n",
    "          self.attention_type = TrainableScaledDotProductAttention\n",
    "          self.attention_layers = nn.ModuleList([self.attention_type(hidden_size, hidden_size, hidden_size) for i in range(num_layers)])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-1ixBMEDEmZ"
   },
   "source": [
    "Declaramos el Lighting Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 186,
     "status": "ok",
     "timestamp": 1733833003565,
     "user": {
      "displayName": "Javier Huertas",
      "userId": "08531127118556167809"
     },
     "user_tz": -60
    },
    "id": "MN2OXyyiDEmZ",
    "outputId": "1a61e912-824d-4a62-aae1-37bccf84ac84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LSTMAttentionRegressor\n",
      "Model Parameters: 17794\n",
      "Model Output Size: torch.Size([64, 2])\n",
      "\n",
      "Model: LSTMAttentionRegressor\n",
      "Model Parameters: 17794\n",
      "Model Output Size: torch.Size([64, 2])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class StockPredictor(pl.LightningModule):\n",
    "    def __init__(self, model, learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters() # guardamos la configuración de hiperparámetros\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = model\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def compute_batch(self, batch, split='train'):\n",
    "        inputs, targets = batch\n",
    "        output = self(inputs)\n",
    "\n",
    "        preds = output.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        loss = self.criterion(preds, targets)\n",
    "        self.log_dict(\n",
    "            {\n",
    "                f'{split}_loss': loss,\n",
    "            },\n",
    "            on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.compute_batch(batch, 'train')\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.compute_batch(batch, 'val')\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.compute_batch(batch, 'test')\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.learning_rate) # self.parameters() son los parámetros del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DNigCJQ18ae0"
   },
   "source": [
    "## Entrenamiento\n",
    "Explora varios parametros y configuraciones, observa que ocurre al entrenamiento con cada mecanismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dH6IrG2y7yNU"
   },
   "outputs": [],
   "source": [
    "# @title Seleccion parametros\n",
    "w = 10 #@param {type:\"integer\"}\n",
    "h = 3 #@param {type:\"integer\"}\n",
    "input_size = 1 #@param {type:\"integer\"}\n",
    "batch_size = 64 #@param {type:\"integer\"}\n",
    "num_layers = 1 #@param {type:\"integer\"}\n",
    "hidden_size = 128 #@param {type:\"integer\"}\n",
    "learning_rate = 1e-3 #@param {type:\"number\"}\n",
    "p_drop = 0.2 #@param {type:\"number\"}\n",
    "pooling = 'last' #@param [\"last\", \"mean\", \"max\"]\n",
    "attention = 'none' #@param [\"none\", \"additive\", \"general\", \"sdpa\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uvpogzW7xfbc"
   },
   "outputs": [],
   "source": [
    "# Parámetros\n",
    "SAVE_DIR = f'lightning_logs/stock/{datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}'\n",
    "\n",
    "# DataModule\n",
    "data = pd.read_csv(DATA_PATH)\n",
    "data_module = StocksDataModule(data, w=w, h=h, batch_size=batch_size)\n",
    "\n",
    "# Model\n",
    "model = AdvancedAttentionLSTMRegressor(h=h, input_size=input_size,\n",
    "                                       hidden_size=hidden_size,\n",
    "                                       num_layers=num_layers,\n",
    "                                       batch_first=True,\n",
    "                                       p_drop=p_drop,\n",
    "                                       pooling=pooling,\n",
    "                                       attention=attention)\n",
    "\n",
    "# LightningModule\n",
    "module = StockPredictor(model, learning_rate=learning_rate)\n",
    "\n",
    "# Callbacks\n",
    "early_stopping_callback = pl.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', # monitorizamos la pérdida en el conjunto de validación\n",
    "    mode='min',\n",
    "    patience=5, # número de epochs sin mejora antes de parar\n",
    "    verbose=False, # si queremos que muestre mensajes del estado del early stopping\n",
    ")\n",
    "model_checkpoint_callback = pd.callbacks.ModelCheckpoint(\n",
    "    monitor='val_loss', # monitorizamos la pérdida en el conjunto de validación\n",
    "    mode='min', # queremos minimizar la pérdida\n",
    "    save_top_k=1, # guardamos solo el mejor modelo\n",
    "    dirpath=SAVE_DIR, # directorio donde se guardan los modelos\n",
    "    filename=f'best_model' # nombre del archivo\n",
    ")\n",
    "\n",
    "# Descomentar en función de si queremos o no el callback de forecasting\n",
    "# forecasting_callback = ForecastingCallback()\n",
    "# callbacks = [early_stopping_callback, model_checkpoint_callback, forecasting_callback]\n",
    "\n",
    "callbacks = [early_stopping_callback, model_checkpoint_callback]\n",
    "\n",
    "# Loggers\n",
    "csv_logger = pl.loggers.CSVLogger(\n",
    "    save_dir=SAVE_DIR,\n",
    "    name='metrics',\n",
    "    version=None\n",
    ")\n",
    "\n",
    "loggers = [csv_logger] # se pueden poner varios loggers (mirar documentación)\n",
    "\n",
    "# Trainer\n",
    "trainer = pl.Trainer(max_epochs=50, accelerator='gpu', devices=[0], callbacks=callbacks, logger=loggers)\n",
    "\n",
    "trainer.fit(module, data_module)\n",
    "results = trainer.test(module, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h880g3GMxkGE"
   },
   "outputs": [],
   "source": [
    "# Parámetros\n",
    "SAVE_DIR = f'lightning_logs/sales/{datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}'\n",
    "w = 50\n",
    "h = 3\n",
    "batch_size = 64\n",
    "num_layers = 1\n",
    "hidden_size = 128\n",
    "learning_rate = 1e-3\n",
    "p_drop = 0.2\n",
    "\n",
    "# DataModule\n",
    "data = pd.read_csv(DATA_PATH)\n",
    "data_module = StockDataModule(data, w=w, h=h, batch_size=batch_size)\n",
    "\n",
    "# Model (Probad a descomentar uno u otro)\n",
    "#lstm = LSTMTrainableAttentionRegressor(...) # Usa esta otra definicion mas adelante\n",
    "lstm = LSTMTrainableAttentionRegressor(h=h,\n",
    "                              input_size=1,\n",
    "                              hidden_size=hidden_size,\n",
    "                              num_layers=num_layers,\n",
    "                              batch_first=True,\n",
    "                              enable_attention=True, # Cambia a false para probar que ocurre!\n",
    "                              )\n",
    "# model = MLPRegressor(w=w, h=h, input_size=1, hidden_size=hidden_size)\n",
    "\n",
    "# LightningModule\n",
    "module = SalesPredictor(lstm, learning_rate=learning_rate)\n",
    "\n",
    "# Callbacks\n",
    "early_stopping_callback = pl.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', # monitorizamos la pérdida en el conjunto de validación\n",
    "    mode='min',\n",
    "    patience=5, # número de epochs sin mejora antes de parar\n",
    "    verbose=False, # si queremos que muestre mensajes del estado del early stopping\n",
    ")\n",
    "model_checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    monitor='val_loss', # monitorizamos la pérdida en el conjunto de validación\n",
    "    mode='min', # queremos minimizar la pérdida\n",
    "    save_top_k=1, # guardamos solo el mejor modelo\n",
    "    dirpath=SAVE_DIR, # directorio donde se guardan los modelos\n",
    "    filename=f'best_model' # nombre del archivo\n",
    ")\n",
    "\n",
    "callbacks = [early_stopping_callback, model_checkpoint_callback]\n",
    "\n",
    "# Loggers\n",
    "csv_logger = pl.loggers.CSVLogger(\n",
    "    save_dir=SAVE_DIR,\n",
    "    name='metrics',\n",
    "    version=None\n",
    ")\n",
    "\n",
    "loggers = [csv_logger] # se pueden poner varios loggers (mirar documentación)\n",
    "\n",
    "# Trainer\n",
    "trainer = pl.Trainer(max_epochs=50, accelerator='cpu', callbacks=callbacks, logger=loggers)\n",
    "\n",
    "trainer.fit(module, data_module)\n",
    "results = trainer.test(module, data_module)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
