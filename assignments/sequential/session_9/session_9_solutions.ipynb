{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Deep Learning - Master in Deep Learning of UPM</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANTE**\n",
    "\n",
    "Antes de empezar debemos instalar PyTorch Lightning, por defecto, esto valdría:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además, si te encuentras ejecutando este código en Google Collab, lo mejor será que montes tu drive para tener acceso a los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pytorch_lightning\n",
    "import torchmetrics\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "DATA_PATH = 'data/stocks.csv'\n",
    "SEED = 42\n",
    "seed_everything(seed=SEED) # Fijamos una semilla para reproducibilidad en los experimentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a volver a enfrentarnos al mismo problema de stock prediction incorporando los conocimientos adquiridos sobre LSTMs y GRUs. Sin embargo, para subir algo el nivel, en este caso será multivariable, siendo en este caso la empresa Google.\n",
    "\n",
    "Nuestra variable objetivo será el precio de cierre de cierta acción en el futuro (Close), sin embargo, ahora tenemos otras variables que representan como ha evolucionado el precio en el transcurso de un día, como son:\n",
    "- Open: en qué precio abrió\n",
    "- Close: en qué precio cerró\n",
    "- High: el valor más alto en el día\n",
    "- Low: el valor más bajo en el día\n",
    "\n",
    "Como ya hemos visto, en series temporales, la misma variable puede ser a la vez inputs y etiquetas, siempre y cuando difieran temporalmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "df.head() # Imprimamos las primeras filas del dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos diferentes variables diarias referentes al precio de la acción de Google a lo largo de aproximadamente 10 años ([más información](https://www.kaggle.com/datasets/szrlee/stock-time-series-20050101-to-20171231/data))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(DATA_PATH)\n",
    "\n",
    "data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')\n",
    "data.sort_values('date', inplace=True)\n",
    "print(f\"date range: {data['date'].min()} to {data['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos la forma de la serie temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot(x='date', y=['open', 'close', 'high', 'low'], title='GOOGL stock price', ylabel='price', xlabel=['open', 'close', 'high', 'low'], figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiene sentido... vamos a analizar una ventana más corta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "# Filtrar los datos para una semana específica\n",
    "start_date = '2008-11-01'  # Fecha de inicio de cierta ventana de tiempo\n",
    "end_date = '2008-12-31'    # Fecha de fin de cierta ventana de tiempo\n",
    "window = data[(data['date'] >= start_date) & (data['date'] <= end_date)]\n",
    "\n",
    "# Graficar los datos filtrados\n",
    "window.plot(x='date', y=['open', 'close', 'high', 'low'], \n",
    "               title=f'GOOGL stock price ({start_date} to {end_date})',\n",
    "               ylabel='price', xlabel='Date',\n",
    "               figsize=(10, 5))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construye un Dataset que reciba la compañía sobre la que se va a entrenar, el tamaño de ventana y de horizonte. Teniendo en cuenta que estamos ante una serie multivariable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StocksDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, w=10, h=1):\n",
    "        self.data = df.drop(['date'], axis=1).values # Eliminamos la columna 'date' y guardamos el resto de columnas como datos\n",
    "        self.target = df['close'].values # Guardamos por separado la columna 'close' que será nuestra variable objetivo\n",
    "        self.w = w # Ventana de tiempo\n",
    "        self.h = h # Horizonte de predicción\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - (self.w + self.h) + 1\n",
    "\n",
    "    def __getitem__(self, t):\n",
    "        features = self.data[t:t+self.w] # [i: i+w)\n",
    "        target = self.target[t+self.w: t+self.w+self.h] # [i+w, i+w+h)\n",
    "        return features, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para seguir, el siguiente código no debe dar error ninguno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 10\n",
    "h = 3\n",
    "input_size = len(['open', 'high', 'low', 'close'])\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "dataset = StocksDataset(df, w=w, h=h)\n",
    "\n",
    "sample = dataset[0]\n",
    "\n",
    "assert len(sample) == 2 # Comprobamos que el dataset devuelve dos elementos\n",
    "assert sample[0].shape == (w, input_size) # Comprobamos que las features tienen la forma correcta\n",
    "assert sample[1].shape == (h,) # Comprobamos que el target tiene la forma correcta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si hacemos MinMax el valor máximo del train (valores más tempranos temporalmente) será muy inferior a los valores máximos del test (futuro). Pudiendo pasar algo como esto:\n",
    "\n",
    "![incorrect_norm_time_series](../../../utils/incorrect_norm_time_series.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una forma de eliminar la tendencia (detrending) es con diferenciación temporal. A cada valor futuro le restaremos el anterior iterativamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "def detrend(df):\n",
    "    # Aplicar la diferenciación\n",
    "    detrended_df = df.copy()\n",
    "    detrended_df[['open', 'high', 'low', 'close']] = df[['open', 'high', 'low', 'close']].diff()\n",
    "\n",
    "    detrended_df.dropna(inplace=True)\n",
    "    return detrended_df\n",
    "\n",
    "detrended_df = detrend(df)\n",
    "\n",
    "# Graficar los datos detrended\t\n",
    "detrended_df.plot(x='date', y=['open', 'close', 'high', 'low'], title='GOOGL stock price', ylabel='price', xlabel=['open', 'close', 'high', 'low'], figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya podemos crear el DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StocksDataModule(pytorch_lightning.LightningDataModule):\n",
    "    def __init__(self, df, w=10, h=1, batch_size=16, val_size=0.2, test_size=0.2):\n",
    "        super().__init__()\n",
    "        self.data = df \n",
    "        self.detrend() # Aplicamos la diferenciación eliminando la tendencia\n",
    "\n",
    "        self.sequential_train_val_test_split(val_size=val_size, test_size=test_size)\n",
    "        self.normalize()\n",
    "\n",
    "        self.w = w\n",
    "        self.h = h\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == 'fit':\n",
    "            self.train_dataset = StocksDataset(self.train_df, w=self.w, h=self.h)\n",
    "            self.val_dataset = StocksDataset(self.val_df, w=self.w, h=self.h)\n",
    "        elif stage == 'test':\n",
    "            self.test_dataset = StocksDataset(self.test_df, w=self.w, h=self.h)\n",
    "\n",
    "    def detrend(self):\n",
    "        # Aplicar la diferenciación\n",
    "        detrended_df = self.data.copy()\n",
    "        detrended_df[['open', 'high', 'low', 'close']] = df[['open', 'high', 'low', 'close']].diff()\n",
    "\n",
    "        detrended_df.dropna(inplace=True)\n",
    "        self.data = detrended_df\n",
    "\n",
    "    def sequential_train_val_test_split(self, val_size=0.2, test_size=0.2):\n",
    "        # Aseguramos el formato de la fecha y ordenamos por ella\n",
    "        self.data['date'] = pd.to_datetime(self.data['date'], format='%Y-%m-%d')\n",
    "        self.data.sort_values('date', inplace=True)\n",
    "\n",
    "        # Calculamos los índices para hacer los splits\n",
    "        n = len(self.data)\n",
    "        train_end = int((1 - val_size - test_size) * n)\n",
    "        val_end = int((1 - test_size) * n)\n",
    "\n",
    "        self.train_df = self.data.iloc[:train_end].copy()\n",
    "        self.val_df = self.data.iloc[train_end:val_end].copy()\n",
    "        self.test_df = self.data.iloc[val_end:].copy()\n",
    "\n",
    "    def normalize(self):\n",
    "        # Ajustar el scaler solo con los datos de diferencias del conjunto de entrenamiento\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.scaler.fit(self.train_df[['close']].values)\n",
    "\n",
    "        # Escalar las diferencias de train, val y test\n",
    "        for df in [self.train_df, self.val_df, self.test_df]:\n",
    "            for col in ['open', 'high', 'low', 'close']:\n",
    "                df[[col]] = self.scaler.transform(df[[col]].values)\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        features, targets = zip(*batch)\n",
    "\n",
    "        features = np.stack(features, axis=0)  # [batch_size, w, input_size]\n",
    "        targets = np.stack(targets, axis=0)    # [batch_size, h, input_size]\n",
    "\n",
    "        features = torch.tensor(features, dtype=torch.float32)\n",
    "        targets = torch.tensor(targets, dtype=torch.float32)\n",
    "        return features, targets\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, collate_fn=self.collate_fn)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, collate_fn=self.collate_fn)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, collate_fn=self.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, crearemos modelos de regresión temporal basados en LSTMs y GRUs. Buenas noticias, la sintáxis es idéntica a la de la RNN básica ya que a pesar de tener una arquitectura ligeramente diferente para la célula, siguen siendo redes recurrentes con las mismas dimensiones de entradas y salidas.\n",
    "\n",
    "La diferencia con el que hemos aprendido en clase será que este reciba un argumento extra _pooling_[str] que podrá tomar tres valores y actuar en consecuencia:\n",
    "\n",
    "- _pooling_ = \"last\" -> tomará la última salida de la RNN\n",
    "- _pooling_ = \"mean\" -> realizará las medias a lo largo del eje temporal\n",
    "- _pooling_ = \"max\" -> computará el máximo en la dimensión temporal\n",
    "\n",
    "Sea cual sea la dimensión de salida no deberá variar\n",
    "\n",
    "Además será un módulo altamente flexible que aceptará el parámetro _rnn_[str] que podrá tomar tres valores (rnn, lstm o gru) y en función de ello se instanciará en el init el modelo pertinente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STR2MODEL = {\n",
    "    'rnn' : nn.RNN,\n",
    "    'lstm': nn.LSTM,\n",
    "    'gru' : nn.GRU\n",
    "}\n",
    "\n",
    "class TimeSeriesRegressor(nn.Module):\n",
    "    \"\"\"\n",
    "    Time Series Regressor model\n",
    "    rnn[str]: tipo de RNN a utilizar (rnn, lstm o gru)\n",
    "    h[int]: horizonte de predicción\n",
    "    input_size[int]: variables de la serie temporal\n",
    "    hidden_size[int]: tamaño de las capas ocultas de la RNN\n",
    "    num_layers[int]: número de capas de la RNN (si > 1, stacking de células RNN)\n",
    "    batch_first[bool]: si el batch_size es la primera dimensión\n",
    "    p_drop[float]: probabilidad de dropout\n",
    "    pooling[str]: tipo de pooling a realizar sobre las salidas de la RNN\n",
    "    \"\"\"\n",
    "    def __init__(self, rnn='rnn', h=1, input_size=1, hidden_size=64, num_layers=1, batch_first=True, p_drop=0.0, pooling='last'):\n",
    "        super().__init__()\n",
    "        self.pooling = pooling\n",
    "        self.rnn = STR2MODEL[rnn](input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=batch_first, dropout=p_drop)\n",
    "        self.out = nn.Linear(hidden_size, h)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        outputs, _ = self.rnn(x) # [batch_size, seq_len, input_size] -> [batch_size, seq_len, hidden_size]\n",
    "        match self.pooling:\n",
    "            case 'last': # Última salida de la LSTM\n",
    "                pooled = outputs[:, -1, :]\n",
    "            case 'mean': # Media de las salidas de la LSTM\n",
    "                pooled = outputs.mean(dim=1)\n",
    "            case 'max': # Máximo de las salidas de la LSTM\n",
    "                pooled = torch.max(outputs, dim=1)\n",
    "\n",
    "        return self.out(pooled) # [batch_size, hidden_size] -> [batch_size, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creemos el LightningModule, muy similar a otros casos en los que hayamos realizado regresión.\n",
    "\n",
    "El único requisito es computar, aparte de la loss pertinente para regresión, la métrica MAPE o Mean Absolute Percentage Error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockPredictor(pytorch_lightning.LightningModule):\n",
    "    def __init__(self, model, learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters() # guardamos la configuración de hiperparámetros\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.mape = torchmetrics.MeanAbsolutePercentageError()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def compute_batch(self, batch, split='train'):\n",
    "        inputs, targets = batch\n",
    "        output = self(inputs)\n",
    "\n",
    "        preds = output.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        loss = self.criterion(preds, targets)\n",
    "        self.log_dict(\n",
    "            {\n",
    "                f'{split}_loss': loss, \n",
    "                f'{split}_mape': self.mape(preds, targets)\n",
    "            }, \n",
    "            on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.compute_batch(batch, 'train')\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.compute_batch(batch, 'val')\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.compute_batch(batch, 'test')\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.learning_rate) # self.parameters() son los parámetros del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A entrenar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros\n",
    "SAVE_DIR = f'lightning_logs/stocks/{datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}'\n",
    "rnn = 'gru'\n",
    "w = 10\n",
    "h = 1\n",
    "input_size = len(['open', 'high', 'low', 'close'])\n",
    "batch_size = 64\n",
    "num_layers = 1\n",
    "hidden_size = 128\n",
    "learning_rate = 1e-3\n",
    "p_drop = 0.2\n",
    "pooling = 'last'\n",
    "\n",
    "# DataModule\n",
    "data = pd.read_csv(DATA_PATH)\n",
    "data_module = StocksDataModule(data, w=w, h=h, batch_size=batch_size)\n",
    "\n",
    "# Model \n",
    "model = TimeSeriesRegressor(rnn=rnn, h=h, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, p_drop=p_drop, pooling=pooling)\n",
    "\n",
    "# LightningModule\n",
    "module = StockPredictor(model, learning_rate=learning_rate)\n",
    "\n",
    "# Callbacks\n",
    "early_stopping_callback = pytorch_lightning.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', # monitorizamos la pérdida en el conjunto de validación\n",
    "    mode='min', \n",
    "    patience=5, # número de epochs sin mejora antes de parar\n",
    "    verbose=False, # si queremos que muestre mensajes del estado del early stopping \n",
    ")\n",
    "model_checkpoint_callback = pytorch_lightning.callbacks.ModelCheckpoint(\n",
    "    monitor='val_loss', # monitorizamos la pérdida en el conjunto de validación\n",
    "    mode='min', # queremos minimizar la pérdida\n",
    "    save_top_k=1, # guardamos solo el mejor modelo\n",
    "    dirpath=SAVE_DIR, # directorio donde se guardan los modelos\n",
    "    filename=f'best_model' # nombre del archivo\n",
    ")\n",
    "\n",
    "callbacks = [early_stopping_callback, model_checkpoint_callback]\n",
    "\n",
    "# Loggers\n",
    "csv_logger = pytorch_lightning.loggers.CSVLogger(\n",
    "    save_dir=SAVE_DIR,\n",
    "    name='metrics',\n",
    "    version=None\n",
    ")\n",
    "\n",
    "loggers = [csv_logger] # se pueden poner varios loggers (mirar documentación)\n",
    "\n",
    "# Trainer\n",
    "trainer = pytorch_lightning.Trainer(max_epochs=50, accelerator='gpu', devices=[0], callbacks=callbacks, logger=loggers)\n",
    "\n",
    "trainer.fit(module, data_module)\n",
    "results = trainer.test(module, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRA: From Scratch LSTM & GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STR2ACT = {\n",
    "    'tanh' : nn.Tanh,\n",
    "    'sigmoid': nn.Sigmoid,\n",
    "    'relu' : nn.ReLU,\n",
    "    'gelu' : nn.GELU\n",
    "}\n",
    "    \n",
    "class LSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    I_t (input_gate): how much of the input is added to the internal state\n",
    "    F_t (forget_gate): whether to keep the current memory state or flush it\n",
    "    O_t (output_gate): wheter the memory cell should influence the output\n",
    "    C_t (cell_gate): how much we take into account new data\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.layers = [self.lstm_layer(input_layer=(l==0))for l in range(num_layers)]\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid() \n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def lstm_layer(self, input_layer=False):\n",
    "        \"\"\"\n",
    "        self.hidden_size * 4 because we use the same linear layer for\n",
    "        the four gates and then split the outputs\n",
    "        \"\"\"\n",
    "        return nn.ModuleDict({\n",
    "            'w_ih': nn.Linear(self.input_size if input_layer else self.hidden_size, self.hidden_size * 4, bias=False),\n",
    "            'w_hh': nn.Linear(self.hidden_size, self.hidden_size * 4),\n",
    "        })\n",
    "\n",
    "    def forward(self, x, h=None):\n",
    "        \"\"\"\n",
    "        In deep LSTMs (num_layers > 1), hidden and internal state are propagated as:\n",
    "        - h(t, l) -> h(t+1, l); h(t, l) -> h(t, l+1)\n",
    "        - c(t, l) -> c(t+1, l); c(t, l) -> c(t, l+1)\n",
    "        \"\"\"\n",
    "        x = x.transpose(0, 1) # [batch_size, seq_len, input_size] -> [seq_len, batch_size, input_size]\n",
    "        seq_len, batch_size, _ = x.size()\n",
    "\n",
    "        if h is None: # by default, hidden and internal states are initialized to 0s\n",
    "            h = x.new_zeros((self.num_layers, batch_size, self.hidden_size)) # same device and dtype as x\n",
    "\n",
    "        h, c = (h, h) # at first, internal state is a copy of the hidden state\n",
    "\n",
    "        output = []\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[t]\n",
    "            for l, layer in enumerate(self.layers):\n",
    "                gates_proj = layer['w_ih'](x_t) + layer['w_hh'](h[l]) # [batch_size, hidden_size * 4]\n",
    "                input_gate, forget_gate, output_gate, cell_gate = gates_proj.chunk(4, dim=1)\n",
    "\n",
    "                input_gate = self.sigmoid(input_gate)\n",
    "                forget_gate = self.sigmoid(forget_gate)\n",
    "                output_gate = self.sigmoid(output_gate)\n",
    "                cell_gate = self.tanh(cell_gate)\n",
    "\n",
    "                c[l] = c[l] * forget_gate + input_gate * cell_gate\n",
    "                h[l] = self.tanh(c[l]) * output_gate\n",
    "                x_t = h[l]\n",
    "\n",
    "            output.append(h[-1])\n",
    "\n",
    "        return torch.stack(output).transpose(0, 1) # [seq_len, batch_size, hidden_size] -> [batch_size, seq_len, hidden_size]\n",
    "        \n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.layers = nn.ModuleList([self.gru_layer(input_layer=(i==0)) for i in range(num_layers)])\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def gru_layer(self, input_layer=False):\n",
    "        \"\"\"\n",
    "        self.hidden_size * 2 because we use the same linear layer for\n",
    "        computing the reset and update gate\n",
    "\n",
    "        w_ch is the linear layer for computing the candidate hidden state\n",
    "        \"\"\"\n",
    "        return nn.ModuleDict({\n",
    "            'w_ig': nn.Linear(self.input_size if input_layer else self.hidden_size, self.hidden_size * 2, bias=False), # input to reset and update gates\n",
    "            'w_hg': nn.Linear(self.hidden_size, self.hidden_size * 2), # hidden state to reset and update gates\n",
    "            'w_ih': nn.Linear(self.input_size if input_layer else self.hidden_size, self.hidden_size, bias=False), # input to candidate hidden state\n",
    "            'w_gh': nn.Linear(self.hidden_size, self.hidden_size) # reset gate * hidden state to candidate hidden state\n",
    "        })\n",
    "    \n",
    "    def forward(self, x, h=None):\n",
    "        x = x.transpose(0, 1) # [batch_size, seq_len, input_size] -> [seq_len, batch_size, input_size]\n",
    "        seq_len, batch_size, _ = x.size()\n",
    "\n",
    "        if h is None: # by default, hidden and internal states are initialized to 0s\n",
    "            h = x.new_zeros((self.num_layers, batch_size, self.hidden_size)) # same device and dtype as x\n",
    "\n",
    "        output = []\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[t]\n",
    "            for l, layer in enumerate(self.layers):\n",
    "                gates_proj = layer['w_ig'](x_t) + layer['w_hg'](h[l])\n",
    "                reset_gate, update_gate = gates_proj.chunk(2, dim=1)\n",
    "\n",
    "                reset_gate = self.sigmoid(reset_gate)\n",
    "                update_gate = self.sigmoid(update_gate)\n",
    "\n",
    "                candidate_hidden_state = layer['w_gh'](reset_gate * h[l]) + layer['w_ih'](x_t)\n",
    "                candidate_hidden_state = self.tanh(candidate_hidden_state)\n",
    "\n",
    "                h[l] = ((1 - update_gate) * candidate_hidden_state) + (h[l] * update_gate)\n",
    "\n",
    "                x_t = h[l]\n",
    "            \n",
    "            output.append(h[-1])\n",
    "\n",
    "        return torch.stack(output).transpose(0, 1) # [seq_len, batch_size, hidden_size] -> [batch_size, seq_len, hidden_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 3\n",
    "hidden_size = 128\n",
    "num_layers = 3\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "\n",
    "\n",
    "model = LSTM(input_size, hidden_size, num_layers=num_layers)\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, input_size)\n",
    "\n",
    "x.size(), model(x).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional RNN\n",
    "La misma lógica es extrapolable a LSTMs o GRUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of a Recurrent Neural Network\n",
    "\n",
    "    __init__()\n",
    "    input_size[int]: features per timestep, if > 1 is multivariate\n",
    "    hidden_size[int]: hidden size, normally between (64, 2056)\n",
    "    num_layers[int]: number of rnn stacked cells, if > 1 is DeepRNN, normally between (1, 8)\n",
    "    bidirectional[bool]: if the rnn is bidirectional\n",
    "    bias[bool]: if input linear layers have bias\n",
    "\n",
    "    forward()\n",
    "    x[torch.Tensor]: model input of size [batch_size, seq_len, input_size]\n",
    "    h[torch.Tensor]: initial hidden state of size [batch_size, seq_len, input_size], if None it will be zeros\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, bidirectional=False, bias=False, activation='tanh', p_drop=0.0):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.bias = bias\n",
    "        self.p_drop = p_drop\n",
    "\n",
    "        self.l2r = nn.ModuleList([self.rnn_layer(input_layer=(l==0)) for l in range(self.num_layers)]) # left to right RNN\n",
    "\n",
    "        if bidirectional:\n",
    "            self.r2l = nn.ModuleList([self.rnn_layer(input_layer=(l==0)) for l in range(self.num_layers)]) # right to left RNN\n",
    "\n",
    "        self.activation = STR2ACT[activation]() # tanh by default\n",
    "\n",
    "\n",
    "    def rnn_layer(self, input_layer=False):\n",
    "        return nn.ModuleDict({\n",
    "            'w_ih' : nn.Linear(self.input_size if input_layer else self.hidden_size, self.hidden_size, bias=self.bias),\n",
    "            'w_hh' : nn.Linear(self.hidden_size, self.hidden_size),\n",
    "            'drop' : nn.Dropout(self.p_drop)\n",
    "        })\n",
    "    \n",
    "    def process_sequence(self, x, h, direction='l2r'):\n",
    "        \"\"\"\n",
    "        In deep RNNs (num_layers > 1), hidden state is propagated as:\n",
    "        - input hidden state of the next layer of the current timestep; h(l, t) -> h(l+1, t)\n",
    "        - input hidden state of the current layer of the next timestep; h(l, t) -> h(l, t+1)\n",
    "        \"\"\"\n",
    "        seq_len = x.size(0)\n",
    "        layers = self.l2r if direction == 'l2r' else self.r2l\n",
    "\n",
    "        output = []\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[t] # input\n",
    "            for l, layer in enumerate(layers):\n",
    "                # h[l] here is the lth hidden state for t-1\n",
    "                # when override it will be the lth hidden state for t\n",
    "                h[l] = self.activation(\n",
    "                    layer['w_ih'](x_t) + layer['w_hh'](h[l])\n",
    "                )\n",
    "                h[l] = layer['drop'](h[l]) # no dropout by default\n",
    "\n",
    "                x_t = h[l] # also, it will be the input for the l+1th layer of t\n",
    "            \n",
    "            output.append(h[-1]) # final hidden state for timestep t\n",
    "        \n",
    "        return torch.stack(output).transpose(0, 1) # [seq_len, batch_size, hidden_size] -> [batch_size, seq_len, hidden_size]\n",
    "\n",
    "    def forward(self, x, h=None):\n",
    "        x = x.transpose(0, 1) # [batch_size, seq_len, input_size] -> [seq_len, batch_size, input_size]\n",
    "        batch_size = x.size(1)\n",
    "\n",
    "        if self.bidirectional:\n",
    "            if h is None: # by default, hidden state is initialized to 0s\n",
    "                h = x.new_zeros((2, self.num_layers, batch_size, self.hidden_size)) # same device and dtype as x\n",
    "            \n",
    "            h_l2r, h_r2l = h[0], h[1] # if h is passed in bidirectional it is assumed that the format is [2, num_layers, batch_size, hidden_size]\n",
    "            \n",
    "            l2r_output = self.process_sequence(x, h_l2r, direction='l2r')\n",
    "            r2l_output = self.process_sequence(x.flip(dims=[-1]), h_r2l, direction='r2l')\n",
    "\n",
    "            r2l_output = r2l_output.flip(dims=[-1]) # flip to get left to right context\n",
    "\n",
    "            output = torch.cat([l2r_output, r2l_output], dim=-1) # [batch_size, seq_len, 2 * hidden_size]\n",
    "\n",
    "        else:\n",
    "            if h is None: # by default, hidden state is initialized to 0s\n",
    "                h = x.new_zeros((self.num_layers, batch_size, self.hidden_size)) # same device and dtype as x\n",
    "            \n",
    "            output = self.process_sequence(x, h, direction='l2r') # [batch_size, seq_len, hidden_size]\n",
    "        \n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
