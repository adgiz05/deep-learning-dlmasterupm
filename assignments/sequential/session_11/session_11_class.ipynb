{"cells":[{"cell_type":"markdown","metadata":{"id":"eXl5PzoKDEmK"},"source":["<h1 align=\"center\">Deep Learning - Master in Deep Learning of UPM</h1>"]},{"cell_type":"markdown","metadata":{"id":"w3uHCp0VDEmM"},"source":["**IMPORTANTE**\n","\n","Antes de empezar debemos instalar PyTorch Lightning, por defecto, esto valdría:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SArzmQSlDEmO"},"outputs":[],"source":["!pip install pytorch-lightning"]},{"cell_type":"markdown","metadata":{"id":"O23KCJuqDEmQ"},"source":["Además, si te encuentras ejecutando este código en Google Collab, lo mejor será que montes tu drive para tener acceso a los datos:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OCtv-uFjDEmQ"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"RLPAFe08DEmR"},"source":["<h1 align=\"center\">Transformers!</h1>"]},{"cell_type":"markdown","metadata":{"id":"Bn-Fa_vQDEmR"},"source":["En esta sesión práctica diseccionaremos un transformer:\n","- MHSDPA: MultiHead Scaled Dot product attention\n","- RoPE: Rotary Positional Embeddings\n","- El bloque transformer\n","- Un transformer completo para visión\n"]},{"cell_type":"markdown","source":["## Carga del dataset\n","Vamos a visitar un viejo conocido: MNIST. Esta vez será algo diferente... veamos como cargarlo."],"metadata":{"id":"nl2-gyQIHVRY"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"C73taijTDEmT"},"outputs":[],"source":["import datetime\n","\n","import torch\n","import torch.nn as nn\n","\n","from torchvision.datasets import MNIST\n","from torchvision import transforms\n","from torch.utils.data import DataLoader, random_split\n","from torchvision.transforms import ToTensor\n","from torchvision import datasets, transforms\n","\n","import pytorch_lightning as pl\n","import torchmetrics\n","from pytorch_lightning import seed_everything\n","\n","import numpy as np\n","\n","import pandas as pd\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","\n","import matplotlib.pyplot as plt\n","\n","from einops.layers.torch import Rearrange\n","\n","seed_everything(42)"]},{"cell_type":"markdown","metadata":{"id":"AScwtwcZDEmX"},"source":["Vamos a crear el directamente el modulo de datos. El dataset venía descrito en `datasets.MNIST` por lo que no vamos a declarar un Dataset."]},{"cell_type":"code","source":["class MNISTDataModule(pl.LightningDataModule):\n","    def __init__(self, batch_size=64):\n","        super().__init__()\n","        self.batch_size = batch_size\n","        # Transformaciones\n","        self.train_transform = transforms.Compose([\n","            transforms.RandAugment(num_ops=3, magnitude=1),\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.1307,), (0.3081,))\n","        ])\n","        self.val_test_transform = transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.1307,), (0.3081,))\n","        ])\n","\n","    def prepare_data(self):\n","        datasets.MNIST(root=\"data\", train=True, download=True)\n","        datasets.MNIST(root=\"data\", train=False, download=True)\n","\n","    def setup(self, stage=None):\n","        if stage in (None, \"fit\"):\n","            mnist_full = datasets.MNIST(root=\"data\", train=True, transform=self.val_test_transform)\n","            self.train_dataset, self.val_dataset = random_split(\n","                mnist_full,\n","                [55000, 5000],\n","                generator=torch.Generator().manual_seed(42)\n","            )\n","            self.train_dataset.dataset.transform = self.train_transform\n","\n","        if stage == \"test\" or stage is None:\n","            self.test_dataset = datasets.MNIST(root=\"data\", train=False, transform=self.val_test_transform)\n","\n","    def train_dataloader(self):\n","        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n","\n","    def val_dataloader(self):\n","        return DataLoader(self.val_dataset, batch_size=self.batch_size)\n","\n","    def test_dataloader(self):\n","        return DataLoader(self.test_dataset, batch_size=self.batch_size)"],"metadata":{"id":"br8ObXFCrrVp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Programando un bloque transformer\n","\n","En torch ya vienen descritos los transformers en `nn.Transformer`, `nn.TransformerEncoder` y `nn.TransformerDecoder`. Pero nosotros vamos a hacer una capa transformer de cero en este bloque. Sigue atentamente el ejemplo!"],"metadata":{"id":"1cIf3GkkMrFI"}},{"cell_type":"code","source":["class MultiHeadScaledDotProductAttention(nn.Module):\n","    \"\"\"\n","    Modulo de atencion multicabezal\n","    hidden_dim[int]: tamaño de la representación\n","    num_heads[int]: número de cabezas de atención\n","    \"\"\"\n","    def __init__(self, hidden_dim, num_heads):\n","        super(MultiHeadScaledDotProductAttention, self).__init__()\n","        self.hidden_dim = hidden_dim\n","        self.num_heads = num_heads\n","\n","        self.head_dim = hidden_dim // num_heads # Debe ser divisible\n","\n","        self.W_q = nn.Linear(hidden_dim, hidden_dim)\n","        self.W_k = nn.Linear(hidden_dim, hidden_dim)\n","        self.W_v = nn.Linear(hidden_dim, hidden_dim)\n","\n","        self.fc_out = nn.Linear(hidden_dim, hidden_dim)\n","\n","    def forward(self, q, k, v):\n","        # q, k, v > [batch_size, seq_len, hidden_dim]\n","        batch_size, seq_len, hidden_dim = q.shape\n","\n","        # Proyecciones lineales\n","        q = self.W_q(q)  # [batch_size, seq_len, hidden_dim]\n","        k = self.W_k(k)  # [batch_size, seq_len, hidden_dim]\n","        v = self.W_v(v)  # [batch_size, seq_len, hidden_dim]\n","\n","        # Separacion en cabezales\n","        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)  # [batch_size, num_heads, seq_len, head_dim]\n","        k = k.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)  # [batch_size, num_heads, seq_len, head_dim]\n","        v = v.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)  # [batch_size, num_heads, seq_len, head_dim]\n","\n","        # Scaled dot-product attention\n","        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim**1/2)  # [batch_size, num_heads, seq_len, seq_len]\n","\n","        attention_weights = torch.nn.functional.softmax(scores, dim=-1)  # [batch_size, num_heads, seq_len, seq_len]\n","        context_vector = torch.matmul(attention_weights, v)  # [batch_size, num_heads, seq_len, head_dim]\n","\n","        # concatenar cabezas (recuperar la forma original)\n","        context_vector = context_vector.transpose(1, 2).contiguous().view(batch_size, seq_len, hidden_dim)  # [batch_size, seq_len, hidden_dim]\n","\n","        # Final linear layer\n","        output = self.fc_out(context_vector)  # [batch_size, seq_len, hidden_dim]\n","\n","        return output, attention_weights"],"metadata":{"id":"ObshSVc_tYqu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Vamos a programar una versión básica del bloque transformer de cero!"],"metadata":{"id":"aGDiF2h5wrFK"}},{"cell_type":"code","source":["class TransformerBlock(nn.Module):\n","    \"\"\"\n","    Transformer Block\n","    hidden_dim [int]: size of the representation\n","    num_heads [int]: number of attention heads\n","    dropout_prob [float]: dropout probability\n","    \"\"\"\n","    def __init__(self, hidden_dim, num_heads, dropout_prob=0.0):\n","        super(TransformerBlock, self).__init__()\n","        self.mhsdpa = MultiHeadScaledDotProductAttention(hidden_dim, num_heads)\n","        self.norm1 = nn.LayerNorm(hidden_dim)\n","        self.norm2 = nn.LayerNorm(hidden_dim)\n","        self.dropout1 = nn.Dropout(dropout_prob)\n","        self.dropout2 = nn.Dropout(dropout_prob)\n","        self.feed_forward = nn.Sequential(\n","            nn.Linear(hidden_dim, 4 * hidden_dim),\n","            nn.GELU(),\n","            nn.Linear(4 * hidden_dim, hidden_dim)\n","        )\n","\n","    def forward(self, x):\n","        # x > [batch_size, seq_len, hidden_dim]\n","        x = self.norm1(x)\n","        attention_output, _ = self.mhsdpa(x, x, x)\n","        x = x + self.dropout1(attention_output)\n","        x = self.norm2(x)\n","        feed_forward_output = self.feed_forward(x)\n","        x = x + self.dropout2(feed_forward_output)\n","        return x"],"metadata":{"id":"SpHnNPhqwqrT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Y con esto ya está el bloque básico programado! Pero aun falta trabajo para conseguir montar todo..."],"metadata":{"id":"E2OGVKtsx-rV"}},{"cell_type":"markdown","source":["## RoPE\n","Vamos a ver los embeddings posicionales. RoPE es complicado, sigue estos pasos.\n","1. Computar los embeddings posicionales. Esto es algo fijo! Viene dado por el tamaño de la secuencia y el tamaño del encoder.\n","2. Usa los embeddings posicionales para computar los embeddings rotados."],"metadata":{"id":"9B1Y64kBwA7_"}},{"cell_type":"code","source":["def get_positional_embeddings(seq_len, dim, base=10000):\n","    # Esta es la frecuencia inversa que usa llama por defecto\n","    inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n","    # Secuencia de elementos, se codifica con floats de 0 a seq_len-1\n","    pos = torch.arange(seq_len, dtype=torch.float).unsqueeze(1)\n","    # Einstein sumation! Nueva funcion, muy util!\n","    # https://pytorch.org/docs/stable/generated/torch.einsum.html\n","    sinusoid_inp = torch.einsum(\"ik,j->ij\", pos, inv_freq)\n","\n","    # Calculo de el seno y coseno de los embeddings, calculamos\n","    # rotaciones geométricas\n","    embeddings = torch.cat((sinusoid_inp.sin(), sinusoid_inp.cos()), dim=-1)\n","    return embeddings\n","\n","def apply_RoPE(x, positional_embeddings):\n","    seq_len, dim = x.shape\n","    # Otra einstein summation, esta vez su proposito es rotar x (bnd), mediante\n","    # positional embeddings (nd) para obtener la rotacion (bnd)\n","    x_rotated = torch.einsum(\"bnd,nd->bnd\", x, positional_embeddings)\n","    return x_rotated"],"metadata":{"id":"w7Br0ijRyRiC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ahora con esas funciones es sencillo declarar un módulo RoPE para nuestro transformer."],"metadata":{"id":"nAH2ASp_1e8X"}},{"cell_type":"code","source":["class RotaryPositionEmbedding(nn.Module):\n","    def __init__(self, max_seq_len, dim, base=10000):\n","        super(RotaryPositionEmbedding, self).__init__()\n","        self.dim = dim\n","        self.base = base\n","        self.max_seq_len = max_seq_len\n","        self.embeddings = self.get_positional_embeddings()\n","\n","    def get_positional_embeddings(self):\n","        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2).float() / self.dim))\n","        pos = torch.arange(self.max_seq_len, dtype=torch.float).unsqueeze(1)\n","        sinusoid_inp = torch.einsum(\"ik,j->ij\", pos, inv_freq)\n","        embeddings = torch.cat((sinusoid_inp.sin(), sinusoid_inp.cos()), dim=-1)\n","        return embeddings\n","\n","    def forward(self, x):\n","        # x: [batch_size, seq_len, dim]\n","        _, seq_len, _ = x.shape\n","        x_rotated = torch.einsum(\"bnd,nd->bnd\", x, self.embeddings[:seq_len].to(x.device))\n","        return x_rotated"],"metadata":{"id":"HOPvcxNdyW5P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finalmente vamos a mejorar nuestro bloque transformer con esta nueva operacion"],"metadata":{"id":"FZnz_XUm4zRe"}},{"cell_type":"code","source":["class TransformerBlockPlus(nn.Module):\n","    \"\"\"\n","    Transformer Block\n","    hidden_dim [int]: size of the representation\n","    num_heads [int]: number of attention heads\n","    dropout_prob [float]: dropout probability\n","    \"\"\"\n","    def __init__(self, hidden_dim, num_heads, dropout_prob=0.0):\n","        super(TransformerBlockPlus, self).__init__()\n","        self.mhsdpa = MultiHeadScaledDotProductAttention(hidden_dim, num_heads)\n","        self.rope = RotaryPositionEmbedding(512, hidden_dim) # 512 es un tamaño arbitrario!\n","        self.norm1 = nn.LayerNorm(hidden_dim)\n","        self.norm2 = nn.LayerNorm(hidden_dim)\n","        self.dropout1 = nn.Dropout(dropout_prob)\n","        self.dropout2 = nn.Dropout(dropout_prob)\n","        self.feed_forward = nn.Sequential(\n","            nn.Linear(hidden_dim, 4 * hidden_dim),\n","            nn.GELU(),\n","            nn.Linear(4 * hidden_dim, hidden_dim)\n","        )\n","\n","    def forward(self, x):\n","        # x > [batch_size, seq_len, hidden_dim]\n","        x = self.norm1(x)\n","        qx, kx = self.rope(x), self.rope(x) # Nuevo!\n","        attention_output, _ = self.mhsdpa(qx, kx, x)\n","        x = x + self.dropout1(attention_output)\n","        x = self.norm2(x)\n","        feed_forward_output = self.feed_forward(x)\n","        x = x + self.dropout2(feed_forward_output)\n","        return x"],"metadata":{"id":"F6Bs0eTi4yj-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Montaje del transformer\n","Nuestro transformer va a recibir imágenes completas que debemos transformar en parches transformados (transformadas linealmente). Vamos a usar una capa totalmente lineal para este propósito.\n","\n","Hay que trocear las imagenes. Usaremos EINOPS: https://einops.rocks/1-einops-basics/\n","\n","Vamos a transformar la imagen con dimensiones [Batch; Color; Altura; Anchura] → [Batch; $(Altura*Anchura)/(AlturaReducida*AnchuraReducida)$; $AlturaReducida*AnchuraReducida*Color$]\n"],"metadata":{"id":"RMeZZbC02sJN"}},{"cell_type":"code","source":["class TransformerEncoder(nn.Module):\n","    \"\"\"\n","    LSTM Regressor model\n","    h[int]: altura de imagen troceada\n","    w[int]: anchura de imagen troceada\n","    c[int]: número de canales de la imagen\n","    hidden_size[int]: tamaño de las capas ocultas de la RNN\n","    p_drop[float]: probabilidad de dropout\n","    output_size[int]: tamaño de la salida de la red (n_classes)\n","    \"\"\"\n","    def __init__(self, h=7, w=7, # Siempre qe la imagen sea divisible por h y w!\n","                 c=1,\n","                 hidden_size=64,\n","                 p_drop=0.0,\n","                 output_size=1,\n","                 ):\n","        super(TransformerEncoder, self).__init__()\n","        self.linproj = nn.Linear(h*w*c, hidden_size)\n","        self.block1 = TransformerBlockPlus(hidden_size, 4, p_drop)\n","        self.block2 = TransformerBlockPlus(hidden_size, 4, p_drop)\n","        self.fc = nn.Linear(hidden_size, output_size)\n","        '''\n","        Esto quiere decir\n","        Dimension b = batch\n","        Dimension c = colores\n","        Dimension (h i) = h*i es la altura original, h es nuestra reducida\n","         se infiere i a partir del resto de informacion\n","        Dimension (w j) = w*j es la altura original, w es nuestra reducida\n","         Se infiere j a partir del resto de informacion\n","        ------\n","        Son transformadas a\n","        Dimension b = batch\n","        Dimension (i j) = i*j es el tamaño de la secuencia (16 en caso MNIST)\n","        Dimension (c h w) = c*h*w son los features planos de la imagen (7*7 en caso de MNIST)\n","        '''\n","        self.crop = Rearrange('b c (h i) (w j) -> b (i j) (c h w)', h=h, w=w)\n","\n","    def forward(self, x):\n","        # x[batch_size; color_channel; realh; realw]\n","        # Queremos transformarlo en\n","        # x[batch_size; seq_len; h*w]\n","        x = self.crop(x)\n","        x = self.linproj(x)\n","        x = self.block1(x)\n","        x = self.block2(x).mean(1) # Mean pooling de todos los embeddings\n","        return self.fc(x) #out[batch_size; output_size]"],"metadata":{"id":"4A3HD1TeU4py"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Entrenamiento\n","Vamos a hacer el entrenamiento, definiendo el modulo de lighting y el resto de detalles faltantes para completar el proceso."],"metadata":{"id":"pt6GsigfgY3n"}},{"cell_type":"code","source":["class MNISTClassifier(pl.LightningModule):\n","    def __init__(self, model, classes=10, learning_rate=1e-3):\n","        super().__init__()\n","        self.save_hyperparameters() # guardamos la configuración de hiperparámetros\n","        self.learning_rate = learning_rate\n","        self.model = model\n","        self.criterion = nn.CrossEntropyLoss()\n","        self.acc = torchmetrics.Accuracy('multiclass', num_classes=classes)\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","    def compute_batch(self, batch, split='train'):\n","        inputs, targets = batch\n","        preds = self(inputs)\n","        targets = targets.view(-1)\n","\n","        loss = self.criterion(preds, targets)\n","        self.log_dict(\n","            {\n","                f'{split}_loss': loss,\n","                f'{split}_acc': self.acc(preds, targets),\n","            },\n","            on_epoch=True, prog_bar=True)\n","\n","        return loss\n","\n","    def training_step(self, batch, batch_idx):\n","        return self.compute_batch(batch, 'train')\n","\n","    def validation_step(self, batch, batch_idx):\n","        return self.compute_batch(batch, 'val')\n","\n","    def test_step(self, batch, batch_idx):\n","        return self.compute_batch(batch, 'test')\n","\n","    def configure_optimizers(self):\n","        return torch.optim.AdamW(self.parameters(), lr=self.learning_rate) # self.parameters() son los parámetros del modelo"],"metadata":{"id":"dDTLcE8pgR50"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Bucle de entrenamiento"],"metadata":{"id":"jp4kq1lmidvQ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gy4p_UO3DEmb"},"outputs":[],"source":["# Parámetros\n","SAVE_DIR = f'lightning_logs/mnistformer/{datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}'\n","w = 7\n","h = 7\n","batch_size = 8\n","hidden_size = 16\n","learning_rate = 1e-3\n","p_drop = 0.2\n","labels = 10\n","\n","# DataModule\n","data_module = MNISTDataModule(batch_size=batch_size)\n","\n","# Model\n","transformer = TransformerEncoder(h=h,\n","                                 w=w, # Siempre qe la imagen sea divisible por h y w!\n","                                 c=1,\n","                                 hidden_size=hidden_size,\n","                                 p_drop=p_drop,\n","                                 output_size=labels,\n","                                )\n","\n","# LightningModule\n","module = MNISTClassifier(transformer, learning_rate=learning_rate, classes=labels)\n","\n","# Callbacks\n","early_stopping_callback = pl.callbacks.EarlyStopping(\n","    monitor='val_loss', # monitorizamos la pérdida en el conjunto de validación\n","    mode='min',\n","    patience=5, # número de epochs sin mejora antes de parar\n","    verbose=False, # si queremos que muestre mensajes del estado del early stopping\n",")\n","model_checkpoint_callback = pl.callbacks.ModelCheckpoint(\n","    monitor='val_loss', # monitorizamos la pérdida en el conjunto de validación\n","    mode='min', # queremos minimizar la pérdida\n","    save_top_k=1, # guardamos solo el mejor modelo\n","    dirpath=SAVE_DIR, # directorio donde se guardan los modelos\n","    filename=f'best_model' # nombre del archivo\n",")\n","\n","callbacks = [early_stopping_callback, model_checkpoint_callback]\n","\n","# Loggers\n","csv_logger = pl.loggers.CSVLogger(\n","    save_dir=SAVE_DIR,\n","    name='metrics',\n","    version=None\n",")\n","\n","loggers = [csv_logger] # se pueden poner varios loggers (mirar documentación)\n","\n","# Trainer\n","trainer = pl.Trainer(max_epochs=50, accelerator='gpu', callbacks=callbacks, logger=loggers)\n","\n","trainer.fit(module, data_module)\n","results = trainer.test(module, data_module)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}