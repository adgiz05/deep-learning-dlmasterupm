{"cells":[{"cell_type":"markdown","metadata":{"id":"eXl5PzoKDEmK"},"source":["<h1 align=\"center\">Deep Learning - Master in Deep Learning of UPM</h1>"]},{"cell_type":"markdown","metadata":{"id":"w3uHCp0VDEmM"},"source":["**IMPORTANTE**\n","\n","Antes de empezar debemos instalar PyTorch Lightning, por defecto, esto valdría:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SArzmQSlDEmO"},"outputs":[],"source":["!pip install pytorch-lightning"]},{"cell_type":"markdown","metadata":{"id":"O23KCJuqDEmQ"},"source":["Además, si te encuentras ejecutando este código en Google Collab, lo mejor será que montes tu drive para tener acceso a los datos:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OCtv-uFjDEmQ"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"RLPAFe08DEmR"},"source":["<h1 align=\"center\">FOOD 101 - Dataset</h1>"]},{"cell_type":"markdown","metadata":{"id":"Bn-Fa_vQDEmR"},"source":["Este un conjunto de datos desafiante de 101 categorías de alimentos, con 101,000 imágenes. Para cada clase, se proporcionan 250 imágenes de prueba revisadas manualmente, así como 750 imágenes de entrenamiento. A propósito, las imágenes de entrenamiento no se limpiaron y, por lo tanto, aún contienen cierta cantidad de ruido. Esto se presenta principalmente en forma de colores intensos y, a veces, **etiquetas incorrectas**. Todas las imágenes se redimensionaron para tener una longitud de lado máxima de **512** píxeles.\n","\n","https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/\n","\n","Primero: Cargar librerias, añade cualquiera que parezca necesaria"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C73taijTDEmT"},"outputs":[],"source":["import datetime\n","\n","import torch\n","import torch.nn as nn\n","\n","from torchvision.datasets import MNIST\n","from torchvision import transforms\n","from torch.utils.data import DataLoader, random_split\n","from torchvision.transforms import ToTensor\n","from torchvision import datasets, transforms\n","\n","import pytorch_lightning as pl\n","import torchmetrics\n","from pytorch_lightning import seed_everything\n","\n","import numpy as np\n","\n","import pandas as pd\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","\n","import matplotlib.pyplot as plt\n","\n","from einops.layers.torch import Rearrange\n","\n","seed_everything(42)"]},{"cell_type":"markdown","metadata":{"id":"AScwtwcZDEmX"},"source":["Como existe `datasets.Food101` podemos cargarlo al data module"]},{"cell_type":"code","source":["# transforms.Normalize([0.485, 0.456, 0.406],\n","#                      [0.229, 0.224, 0.225])\n","\n","class Food101DataModule(pl.LightningDataModule):\n","    def __init__(self, batch_size=64):\n","        super().__init__()\n","        self.batch_size = batch_size\n","        # Transformaciones\n","        self.train_transform = transforms.Compose([\n","            transforms.Resize((512, 512)),\n","            transforms.RandAugment(num_ops=3, magnitude=1),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.485, 0.456, 0.406],\n","                                 [0.229, 0.224, 0.225])\n","        ])\n","        self.val_test_transform = transforms.Compose([\n","            transforms.Resize((512, 512)),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.485, 0.456, 0.406],\n","                                 [0.229, 0.224, 0.225])\n","        ])\n","\n","    def prepare_data(self):\n","        datasets.Food101(root=\"data\", split='train', download=True)\n","        datasets.Food101(root=\"data\", split='test', download=True)\n","\n","    def setup(self, stage=None):\n","        if stage in (None, \"fit\"):\n","            food_full = datasets.Food101(root=\"data\", split='train',\n","                                         transform=self.val_test_transform)\n","            self.train_dataset, self.val_dataset = random_split(\n","                food_full,\n","                [70000, 5750],\n","                generator=torch.Generator().manual_seed(42)\n","            )\n","            self.train_dataset.dataset.transform = self.train_transform\n","\n","        if stage == \"test\" or stage is None:\n","            self.test_dataset = datasets.Food101(root=\"data\", split='test',\n","                                                 transform=self.val_test_transform)\n","\n","    def train_dataloader(self):\n","        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n","\n","    def val_dataloader(self):\n","        return DataLoader(self.val_dataset, batch_size=self.batch_size)\n","\n","    def test_dataloader(self):\n","        return DataLoader(self.test_dataset, batch_size=self.batch_size)"],"metadata":{"id":"br8ObXFCrrVp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Rehaciendo el transformer\n","\n","Ahora, monta tu propio transformer, usa la clase `RotaryPositionEmbedding` y `nn.MultiheadAttention` para construir la nueva clase `TransformerEncoderPlus`\n","\n","https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html"],"metadata":{"id":"1cIf3GkkMrFI"}},{"cell_type":"code","source":["class RotaryPositionEmbedding(nn.Module):\n","    def __init__(self, max_seq_len, dim, base=10000):\n","        super(RotaryPositionEmbedding, self).__init__()\n","        self.dim = dim\n","        self.base = base\n","        self.max_seq_len = max_seq_len\n","        self.embeddings = self.get_positional_embeddings()\n","\n","    def get_positional_embeddings(self):\n","        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2).float() / self.dim))\n","        pos = torch.arange(self.max_seq_len, dtype=torch.float).unsqueeze(1)\n","        sinusoid_inp = torch.einsum(\"ik,j->ij\", pos, inv_freq)\n","        embeddings = torch.cat((sinusoid_inp.sin(), sinusoid_inp.cos()), dim=-1)\n","        return embeddings\n","\n","    def forward(self, x):\n","        # x: [batch_size, seq_len, dim]\n","        _, seq_len, _ = x.shape\n","        x_rotated = torch.einsum(\"bnd,nd->bnd\", x, self.embeddings[:seq_len].to(x.device))\n","        return x_rotated"],"metadata":{"id":"yfCacLUgeZtQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Primero, rehacer el bloque transformer usando `nn.MultiheadAttention`"],"metadata":{"id":"t26FfJJke1Kl"}},{"cell_type":"code","source":["class TransformerBlockPlus(nn.Module):\n","    \"\"\"\n","    Transformer Block\n","    hidden_dim [int]: size of the representation\n","    num_heads [int]: number of attention heads\n","    dropout_prob [float]: dropout probability\n","    \"\"\"\n","    def __init__(self, hidden_dim, num_heads, dropout_prob=0.0, **kwargs):\n","        super(TransformerBlockPlus, self).__init__()\n","        ...\n","\n","    def forward(self, x):\n","        # x > [batch_size, seq_len, hidden_dim]\n","        ...\n","\n","        return x"],"metadata":{"id":"xqlIz9Jde0MV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finalmente, ponemos todo en comun de nuevo... esta vez generalizando a n capas. Esta vez necesitaremos una red más grande."],"metadata":{"id":"sStsGRnWfPsK"}},{"cell_type":"code","source":["class TransformerEncoderPlus(nn.Module):\n","    \"\"\"\n","    LSTM Regressor model\n","    h[int]: altura de imagen troceada\n","    w[int]: anchura de imagen troceada\n","    c[int]: número de canales de la imagen\n","    hidden_size[int]: tamaño de las capas ocultas de la RNN\n","    heads[int]: número de cabezas de atención\n","    blocks[int]: número de bloques transformer\n","    p_drop[float]: probabilidad de dropout\n","    output_size[int]: tamaño de la salida de la red (n_classes)\n","    \"\"\"\n","    def __init__(self, h=7, w=7, # Siempre qe la imagen sea divisible por h y w!\n","                 c=1,\n","                 hidden_size=64,\n","                 heads=4,\n","                 blocks=1,\n","                 p_drop=0.0,\n","                 output_size=1,\n","                 ):\n","        super(TransformerEncoderPlus, self).__init__()\n","        ...\n","\n","    def forward(self, x):\n","        ...\n","        return x #out[batch_size; output_size]"],"metadata":{"id":"1FhzXX1Yfdm3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Entrenamiento\n","Vamos a hacer el entrenamiento, definiendo el modulo de lighting y el propio bucle de entrenamiento."],"metadata":{"id":"pt6GsigfgY3n"}},{"cell_type":"code","source":["class Food101Classifier(pl.LightningModule):\n","    def __init__(self, model, classes=10, learning_rate=1e-3):\n","        super().__init__()\n","        self.save_hyperparameters(ignore=['model']) # guardamos la configuración de hiperparámetros\n","        self.learning_rate = learning_rate\n","        self.model = model\n","        self.criterion = nn.CrossEntropyLoss()\n","        self.acc = torchmetrics.Accuracy('multiclass', num_classes=classes)\n","        self.f1score = torchmetrics.F1Score(task=\"multiclass\", num_classes=classes)\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","    def compute_batch(self, batch, split='train'):\n","        inputs, targets = batch\n","        preds = self(inputs)\n","        targets = targets.view(-1)\n","\n","        loss = self.criterion(preds, targets)\n","        self.log_dict(\n","            {\n","                f'{split}_loss': loss,\n","                f'{split}_acc': self.acc(preds, targets),\n","                f'{split}_f1': self.f1score(preds, targets)\n","            },\n","            on_epoch=True, prog_bar=True)\n","\n","        return loss\n","\n","    def training_step(self, batch, batch_idx):\n","        return self.compute_batch(batch, 'train')\n","\n","    def validation_step(self, batch, batch_idx):\n","        return self.compute_batch(batch, 'val')\n","\n","    def test_step(self, batch, batch_idx):\n","        return self.compute_batch(batch, 'test')\n","\n","    def configure_optimizers(self):\n","        return torch.optim.AdamW(self.parameters(), lr=self.learning_rate) # self.parameters() son los parámetros del modelo"],"metadata":{"id":"dDTLcE8pgR50"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Parametros del modelo\n","w = 32 #@param {type:\"integer\"}\n","h = 32 #@param {type:\"integer\"}\n","batch_size = 64 #@param {type:\"integer\"}\n","hidden_size = 1024 #@param {type:\"integer\"}\n","heads = 16 #@param {type:\"integer\"}\n","blocks = 6 #@param {type:\"integer\"}\n","learning_rate = 1e-3 #@param {type:\"number\"}\n","p_drop = 0.1 #@param {type:\"number\"}\n"],"metadata":{"id":"pUTV_Qdqm2UB"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gy4p_UO3DEmb"},"outputs":[],"source":["SAVE_DIR = f'lightning_logs/mnistformer/{datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}'\n","labels = 101\n","# DataModule\n","data_module = Food101DataModule(batch_size=batch_size)\n","\n","# Model\n","transformer = TransformerEncoderPlus(h=h,\n","                                      w=w, # Siempre qe la imagen sea divisible por h y w!\n","                                      c=3,\n","                                      heads=heads,\n","                                      blocks=blocks,\n","                                      hidden_size=hidden_size,\n","                                      p_drop=p_drop,\n","                                      output_size=labels,\n","                                      )\n","\n","# LightningModule\n","module = Food101Classifier(transformer, learning_rate=learning_rate, classes=labels)\n","\n","# Callbacks\n","early_stopping_callback = pl.callbacks.EarlyStopping(\n","    monitor='val_loss', # monitorizamos la pérdida en el conjunto de validación\n","    mode='min',\n","    patience=5, # número de epochs sin mejora antes de parar\n","    verbose=False, # si queremos que muestre mensajes del estado del early stopping\n",")\n","model_checkpoint_callback = pl.callbacks.ModelCheckpoint(\n","    monitor='val_loss', # monitorizamos la pérdida en el conjunto de validación\n","    mode='min', # queremos minimizar la pérdida\n","    save_top_k=1, # guardamos solo el mejor modelo\n","    dirpath=SAVE_DIR, # directorio donde se guardan los modelos\n","    filename=f'best_model' # nombre del archivo\n",")\n","\n","callbacks = [early_stopping_callback, model_checkpoint_callback]\n","\n","# Loggers\n","csv_logger = pl.loggers.CSVLogger(\n","    save_dir=SAVE_DIR,\n","    name='metrics',\n","    version=None\n",")\n","\n","loggers = [csv_logger] # se pueden poner varios loggers (mirar documentación)\n","\n","# Trainer\n","trainer = pl.Trainer(max_epochs=50, accelerator='gpu',\n","                     callbacks=callbacks, logger=loggers,\n","                     precision='16-mixed')\n","\n","trainer.fit(module, data_module)\n","results = trainer.test(module, data_module)"]},{"cell_type":"markdown","source":["Nota: Este dataset es DIFICIL, es posible que no aprenda! Intentad lo mejor que podais, a veces esto ocurre."],"metadata":{"id":"OsqYOn1inILt"}}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}