{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNBbFrnXuWgrEanVerI4d+a"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Tema 2: PyTorch - Autograd, MLP con iris\n"],"metadata":{"id":"QVBkmzv-TiGP"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"JaoS5unNTgXb","executionInfo":{"status":"ok","timestamp":1732616668319,"user_tz":-60,"elapsed":10133,"user":{"displayName":"helena liz","userId":"14461101084112642621"}}},"outputs":[],"source":["import torch"]},{"cell_type":"markdown","source":["##1.1. Introducción a Autograd en PyTorch"],"metadata":{"id":"jiuD22SODGr2"}},{"cell_type":"markdown","source":["### Ejercicio 1: Calcular el gradiente de la siguiente función para una sola variable."],"metadata":{"id":"yfmVuumwDWIL"}},{"cell_type":"markdown","source":["a)\n","\n","$$ x = 2$$\n","\n","$$ y = x^{3}$$"],"metadata":{"id":"ekjysJiVmHse"}},{"cell_type":"code","source":["x = torch.tensor(2.0, requires_grad=True)\n","y = x ** 3\n","y.backward()\n","print(x.grad)"],"metadata":{"id":"CuL9w8EHLSQB","executionInfo":{"status":"ok","timestamp":1732616668319,"user_tz":-60,"elapsed":5,"user":{"displayName":"helena liz","userId":"14461101084112642621"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9b6f3c44-8873-4b44-e0d4-833bc6ad2a68"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(12.)\n"]}]},{"cell_type":"markdown","source":["b)\n","\n","$$ x = 5$$\n","\n","$$ y = 2x$$"],"metadata":{"id":"JP1th-M6m0Px"}},{"cell_type":"code","source":["x = torch.tensor(5.0, requires_grad=True)\n","y = 2 * x\n","y.backward()\n","print(x.grad)"],"metadata":{"id":"XZHbfSdVLTeo","executionInfo":{"status":"ok","timestamp":1732616668319,"user_tz":-60,"elapsed":3,"user":{"displayName":"helena liz","userId":"14461101084112642621"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"5cbed56f-5828-4ace-9764-f05bb05c35d5"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(2.)\n"]}]},{"cell_type":"markdown","source":["### Ejercicio 2: Función con Múltiples Variables\n","\n","1. Crear dos tensores, como en ejercicio anterior.\n","2. Definir una operación de multiples variables.\n","3. Calcular el gradiente en función de los dos tensores\n","4. Mostrar los gradientes respecto a los tensores."],"metadata":{"id":"oHGd_7iIVDpS"}},{"cell_type":"markdown","source":["a)\n","\n","$$ x = 2 $$\n","$$ z = 3 $$\n","\n","\n","$$ f = 3x^{2}+z^{3} $$\n"],"metadata":{"id":"h2AvQDzwnCHw"}},{"cell_type":"code","source":["x = torch.tensor(2.0, requires_grad=True)\n","z = torch.tensor(3.0, requires_grad=True)\n","\n","f = 3 * x ** 2 + z ** 3\n","f.backward()\n","\n","\n","print(f\"df/dx = {x.grad}\")\n","print(f\"df/dz = {z.grad}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oi_qRLiCVUEx","executionInfo":{"status":"ok","timestamp":1732616668319,"user_tz":-60,"elapsed":2,"user":{"displayName":"helena liz","userId":"14461101084112642621"}},"outputId":"87f25b0c-21e7-4361-ec3e-0804a59f69e0"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["df/dx = 12.0\n","df/dz = 27.0\n"]}]},{"cell_type":"markdown","source":["### Ejercicio 3: Simulación de una Función de Pérdida y Reinicio de Gradientes, utilizando *grad.zero_()*\n","\n","1. Crear un tensor, con los valores mostrados abajo.\n","2. crear la función de pérdida.\n","3. Calcular el gradiente e imprimirlo.\n","4. Reiniciar el gradiente.\n","5. Crear la nueva función de pérdida.\n","6. Calcular e imprimir los nuevos gradientes.\n"],"metadata":{"id":"8BQ7tQvJeBVs"}},{"cell_type":"markdown","source":["$$ x = [1.0,2.0,3.0]$$\n","\n","$$\\text{Función de pérdida (antes de reiniciar los gradientes)} = (\\sum{x} - 5)^{2}$$\n","\n","$$\\text{Función de pérdida (después de reiniciar los gradientes)} = (\\sum{x} - 6)^{2}$$\n","\n","\n","\n"],"metadata":{"id":"x29PKaC3nyDa"}},{"cell_type":"code","source":["# Crear un tensor de datos\n","x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n","\n","# Definir una función de pérdida como suma cuadrática\n","loss = (x.sum() - 5) ** 2\n","\n","# Calcular gradiente\n","loss.backward()\n","print(\"Gradientes iniciales:\", x.grad)\n","\n","# Reiniciar gradientes y volver a calcular\n","x.grad.zero_()\n","loss = (x.sum() - 6) ** 2\n","loss.backward()\n","print(\"Gradientes después de reiniciar:\", x.grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6krFjiR-LaSR","executionInfo":{"status":"ok","timestamp":1732616668547,"user_tz":-60,"elapsed":2,"user":{"displayName":"helena liz","userId":"14461101084112642621"}},"outputId":"e3986091-f324-4d3f-850d-9efc459f72e7"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Gradientes iniciales: tensor([2., 2., 2.])\n","Gradientes después de reiniciar: tensor([0., 0., 0.])\n"]}]},{"cell_type":"markdown","source":["### Ejercicio 4: Simulación de una Función de Pérdida y Reinicio de Gradientes, utilizando *zero_grad()*\n","\n","1. Crear un tensor, con los valores mostrados abajo.\n","2. Crear el optimizador, utilizando SGD y un learning rate de 0.1.\n","2. Crear la función de pérdida.\n","3. Calcular el gradiente e imprimirlo.\n","4. Reiniciar el gradiente.\n","5. Crear la nueva función de pérdida.\n","6. Calcular e imprimir los nuevos gradientes.\n"],"metadata":{"id":"Ly1HPWwzg45k"}},{"cell_type":"markdown","source":["$$ x = [1.0,2.0,3.0]$$\n","\n","$$\\text{Función de pérdida (antes de reiniciar los gradientes)} = (\\sum{x} - 5)^{2}$$\n","\n","$$\\text{Función de pérdida (después de reiniciar los gradientes)} = (\\sum{x} - 6)^{2}$$\n"],"metadata":{"id":"AuzsSzg7qYKy"}},{"cell_type":"code","source":["# Crear un tensor de datos\n","x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n","\n","# Crear un optimizador\n","optimizer = torch.optim.SGD([x], lr=0.1)\n","\n","# Primera pérdida y gradientes\n","loss = (x.sum() - 5) ** 2\n","loss.backward()\n","print(\"Gradientes iniciales:\", x.grad)\n","\n","# Reiniciar gradientes usando optimizer.zero_grad()\n","optimizer.zero_grad()\n","\n","# Segunda pérdida y gradientes\n","loss = (x.sum() - 6) ** 2\n","loss.backward()\n","print(\"Gradientes después de reiniciar:\", x.grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iR_ibk67g5te","executionInfo":{"status":"ok","timestamp":1732616682257,"user_tz":-60,"elapsed":13711,"user":{"displayName":"helena liz","userId":"14461101084112642621"}},"outputId":"532bc56d-00cd-4ba3-f995-2c7e715a9cba"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Gradientes iniciales: tensor([2., 2., 2.])\n","Gradientes después de reiniciar: tensor([0., 0., 0.])\n"]}]},{"cell_type":"markdown","source":["### Ejercicio 5: Optimización simple usando descenso de gradiente.\n","\n","Crear un buble de 5 épocas que actualice los valores de un tensor para minimizar la función de pérdida.\n","\n","1. Definir el learning rate y x =5.\n","2. Crear un bucle de 5 épocas.\n","3. Calcular la función de pérdida.\n","\n","$$ loss = x^{2}$$\n","\n","4. Imprimir el valor del loss y x.\n","5. Calcular el gradiente.\n","6. Con el cálculo de gradiente desactivado recalcular x con la siguiente función.\n","\n","$$ x = x - \\text{learning_rate} * \\text{x.grad} $$"],"metadata":{"id":"jKOoxz0djXN9"}},{"cell_type":"code","source":["import torch\n","learning_rate = 0.1\n","x = torch.tensor(5.0, requires_grad=True)\n","\n","\n","for epoch in range(5):\n","    loss = x ** 2\n","    print(f\"Epoch {epoch+1}, Pérdida: {loss.item():.2f}, x: {x.item():.2f}\")\n","\n","    loss.backward()\n","\n","    with torch.no_grad():  # Actualización manual sin rastreo\n","\n","        x -= learning_rate * x.grad\n","\n","    x.grad.zero_()  # Reiniciar gradientes"],"metadata":{"id":"ckjfkrQ-ObKX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732616682257,"user_tz":-60,"elapsed":3,"user":{"displayName":"helena liz","userId":"14461101084112642621"}},"outputId":"a439dee0-6642-486b-9fa2-ee414d97ef94"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Pérdida: 25.00, x: 5.00\n","Epoch 2, Pérdida: 16.00, x: 4.00\n","Epoch 3, Pérdida: 10.24, x: 3.20\n","Epoch 4, Pérdida: 6.55, x: 2.56\n","Epoch 5, Pérdida: 4.19, x: 2.05\n"]}]},{"cell_type":"markdown","source":["### Ejercicio 6: Repite el ejercicio anterior utilizando *zero_grad()* y el mismo optimizador del ejercicio 4."],"metadata":{"id":"fV5z-PLXu4MG"}},{"cell_type":"code","source":["x = torch.tensor(5.0, requires_grad=True)\n","\n","# Definir el optimizador\n","learning_rate = 0.1\n","optimizer = torch.optim.SGD([x], lr=learning_rate)\n","\n","# Bucle de optimización\n","for epoch in range(5):\n","    # Calcular la pérdida (ejemplo: minimizar x^2)\n","    loss = x ** 2\n","\n","    # Mostrar el progreso\n","    print(f\"Epoch {epoch+1}, Pérdida: {loss.item():.2f}, x: {x.item():.2f}\")\n","\n","    # Calcular gradientes\n","    loss.backward()\n","\n","    # Actualizar parámetros con el optimizador\n","    optimizer.step()\n","\n","    # Reiniciar gradientes con optimizer.zero_grad()\n","    optimizer.zero_grad()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ygAzOQFlhNr0","executionInfo":{"status":"ok","timestamp":1732616682257,"user_tz":-60,"elapsed":2,"user":{"displayName":"helena liz","userId":"14461101084112642621"}},"outputId":"4487b4d6-d0b2-4273-859e-46499a627f2f"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Pérdida: 25.00, x: 5.00\n","Epoch 2, Pérdida: 16.00, x: 4.00\n","Epoch 3, Pérdida: 10.24, x: 3.20\n","Epoch 4, Pérdida: 6.55, x: 2.56\n","Epoch 5, Pérdida: 4.19, x: 2.05\n"]}]},{"cell_type":"markdown","source":["## 1.2. Crear un MLP con pytorch\n","\n","En este apartado vamos a crear una clase que se llame MLP_pytorch para clasificar el dataset Iris."],"metadata":{"id":"amKvzJOPzeu7"}},{"cell_type":"markdown","source":["Esta **MLP** tiene:\n","- 3 capas densas 64, 16, 3, respectivamente.\n","- La función de activación RELU\n","\n","El **dataset Iris** consta de 150 muestras pertenecientes a 3 clases diferentes. Este dataset dispone de 4 feautres diferentes.\n","\n","\n","\n"],"metadata":{"id":"hj8EYrYN1Pfm"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import accuracy_score"],"metadata":{"id":"N2KRGNPZvcs1","executionInfo":{"status":"ok","timestamp":1732616687600,"user_tz":-60,"elapsed":5344,"user":{"displayName":"helena liz","userId":"14461101084112642621"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["### 1. Cargar el dataset Iris utilizando la librería sklearn"],"metadata":{"id":"8K24K-os9xW8"}},{"cell_type":"code","source":["iris = load_iris()"],"metadata":{"id":"oGwBNUV33imk","executionInfo":{"status":"ok","timestamp":1732616687601,"user_tz":-60,"elapsed":5,"user":{"displayName":"helena liz","userId":"14461101084112642621"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["### 2. Separar las muestras y las etiquetas."],"metadata":{"id":"k6bNK74r94nA"}},{"cell_type":"code","source":["X, y = iris.data, iris.target"],"metadata":{"id":"pFJCI4ej3lt6","executionInfo":{"status":"ok","timestamp":1732616687601,"user_tz":-60,"elapsed":5,"user":{"displayName":"helena liz","userId":"14461101084112642621"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["### 3. Separar las muestras y etiquetas entre: Entrenamiento, Validación y Test\n","\n","Los porcentajes tiene que ser: 30% test, del 80% restante 80% será entrenamiento y 20% será validación."],"metadata":{"id":"Bn3dpsRi9-JH"}},{"cell_type":"code","source":["X_train_tmp, X_test, y_train_tmp, y_test = train_test_split(X, y, train_size=0.8)\n"],"metadata":{"id":"ii2nwbuk3j9C","executionInfo":{"status":"ok","timestamp":1732616687601,"user_tz":-60,"elapsed":5,"user":{"displayName":"helena liz","userId":"14461101084112642621"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["X_train, X_val, y_train, y_val = train_test_split(X_train_tmp, y_train_tmp, train_size=0.8)\n"],"metadata":{"id":"BA0fi8q-31Fe","executionInfo":{"status":"ok","timestamp":1732616687601,"user_tz":-60,"elapsed":4,"user":{"displayName":"helena liz","userId":"14461101084112642621"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["### 4. Transformar los datos en tensores.\n","\n","Las muestras tiene que ser de tipo float y las etiquetas de tipo long"],"metadata":{"id":"s6FDw1jL-IOa"}},{"cell_type":"code","source":["X_tr_tensor = torch.tensor(X_train, dtype=torch.float32)\n","y_tr_tensor = torch.tensor(y_train, dtype=torch.long)\n","\n","X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n","y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n","\n","X_te_tensor = torch.tensor(X_test, dtype=torch.float32)\n","y_te_tensor = torch.tensor(y_test, dtype=torch.long)\n"],"metadata":{"id":"3BVch4IM4DZS","executionInfo":{"status":"ok","timestamp":1732616687601,"user_tz":-60,"elapsed":4,"user":{"displayName":"helena liz","userId":"14461101084112642621"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["### 5. Crear el modelo"],"metadata":{"id":"JEcHWDNxBjTL"}},{"cell_type":"code","source":["class MLP_pytorch(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.l1 = nn.Linear(4, 64)\n","        self.l2 = nn.Linear(64, 16)\n","        self.l3 = nn.Linear(16, 3)\n","        self.act = nn.ReLU()\n","\n","    def forward(self, x):\n","        # Forward pass\n","        x = self.l1(x)\n","        x = self.act(x)\n","        x = self.l2(x)\n","\n","        x = self.act(x)\n","        x = self.l3(x)\n","        return x"],"metadata":{"id":"J7nJGzvo4HiN","executionInfo":{"status":"ok","timestamp":1732616687601,"user_tz":-60,"elapsed":4,"user":{"displayName":"helena liz","userId":"14461101084112642621"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["model = MLP_pytorch()"],"metadata":{"id":"NkInC0cS6Ps9","executionInfo":{"status":"ok","timestamp":1732617810254,"user_tz":-60,"elapsed":225,"user":{"displayName":"helena liz","userId":"14461101084112642621"}}},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":["### 6. Definir los parámetros para el entrenamiento.\n","\n","- Número de épocas\n","- Learning rate\n","- Función de pérdida\n","- Optimizador: Utilizaremos Adams y los parámetros del modelo."],"metadata":{"id":"vmn6s6EhBoFC"}},{"cell_type":"code","source":["epochs = 100\n","learning_rate = 0.001\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"],"metadata":{"id":"9Il-p2pj6ESA","executionInfo":{"status":"ok","timestamp":1732617938154,"user_tz":-60,"elapsed":248,"user":{"displayName":"helena liz","userId":"14461101084112642621"}}},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":["### 7. Entrenar el modelo.\n","\n","En cada época se debe imprimir el Loss y Acc del conjunto de entrenamiento y el conjunto de validación."],"metadata":{"id":"jWhYV8xRCveX"}},{"cell_type":"code","source":["for epoch in range(epochs):\n","\n","  model.train()\n","  optimizer.zero_grad()\n","  output = model(X_tr_tensor)\n","\n","\n","  loss = criterion(output, y_tr_tensor)\n","  loss.backward()\n","  optimizer.step()\n","\n","  _, predicted_tr = torch.max(output, 1)\n","\n","  accuracy = accuracy_score(y_train, predicted_tr)\n","\n","  print(f\"Epoch [{epoch+1}/{epochs}], Loss training: {loss.item():.2f}, Accuracy training: {accuracy:.2f}\")\n","\n","  model.eval()\n","  with torch.no_grad():\n","    val_output = model(X_val_tensor)\n","    val_loss = criterion(val_output, y_val_tensor)\n","    _, predicted_val = torch.max(val_output, 1)\n","    val_accuracy = accuracy_score(y_val_tensor, predicted_val)\n","\n","    print(f\"Epoch [{epoch+1}/{epochs}], Loss validation: {val_loss.item():.2f}, Accuracy validation: {val_accuracy:.2f}\")\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u1y84XW56J8_","executionInfo":{"status":"ok","timestamp":1732617940857,"user_tz":-60,"elapsed":2430,"user":{"displayName":"helena liz","userId":"14461101084112642621"}},"outputId":"c6f9c679-dd64-4638-f4d3-aeb8501ceaff"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/100], Loss training: 0.73, Accuracy training: 0.73\n","Epoch [1/100], Loss validation: 0.67, Accuracy validation: 0.92\n","Epoch [2/100], Loss training: 0.72, Accuracy training: 0.76\n","Epoch [2/100], Loss validation: 0.66, Accuracy validation: 0.92\n","Epoch [3/100], Loss training: 0.71, Accuracy training: 0.78\n","Epoch [3/100], Loss validation: 0.66, Accuracy validation: 0.92\n","Epoch [4/100], Loss training: 0.70, Accuracy training: 0.78\n","Epoch [4/100], Loss validation: 0.65, Accuracy validation: 0.92\n","Epoch [5/100], Loss training: 0.70, Accuracy training: 0.78\n","Epoch [5/100], Loss validation: 0.64, Accuracy validation: 0.92\n","Epoch [6/100], Loss training: 0.69, Accuracy training: 0.79\n","Epoch [6/100], Loss validation: 0.63, Accuracy validation: 0.92\n","Epoch [7/100], Loss training: 0.68, Accuracy training: 0.84\n","Epoch [7/100], Loss validation: 0.63, Accuracy validation: 0.92\n","Epoch [8/100], Loss training: 0.67, Accuracy training: 0.85\n","Epoch [8/100], Loss validation: 0.62, Accuracy validation: 0.92\n","Epoch [9/100], Loss training: 0.67, Accuracy training: 0.86\n","Epoch [9/100], Loss validation: 0.61, Accuracy validation: 0.92\n","Epoch [10/100], Loss training: 0.66, Accuracy training: 0.86\n","Epoch [10/100], Loss validation: 0.60, Accuracy validation: 0.92\n","Epoch [11/100], Loss training: 0.65, Accuracy training: 0.86\n","Epoch [11/100], Loss validation: 0.60, Accuracy validation: 0.92\n","Epoch [12/100], Loss training: 0.64, Accuracy training: 0.86\n","Epoch [12/100], Loss validation: 0.59, Accuracy validation: 0.92\n","Epoch [13/100], Loss training: 0.64, Accuracy training: 0.86\n","Epoch [13/100], Loss validation: 0.58, Accuracy validation: 0.92\n","Epoch [14/100], Loss training: 0.63, Accuracy training: 0.86\n","Epoch [14/100], Loss validation: 0.57, Accuracy validation: 0.92\n","Epoch [15/100], Loss training: 0.62, Accuracy training: 0.86\n","Epoch [15/100], Loss validation: 0.57, Accuracy validation: 0.92\n","Epoch [16/100], Loss training: 0.62, Accuracy training: 0.86\n","Epoch [16/100], Loss validation: 0.56, Accuracy validation: 0.92\n","Epoch [17/100], Loss training: 0.61, Accuracy training: 0.89\n","Epoch [17/100], Loss validation: 0.55, Accuracy validation: 0.92\n","Epoch [18/100], Loss training: 0.60, Accuracy training: 0.90\n","Epoch [18/100], Loss validation: 0.55, Accuracy validation: 0.92\n","Epoch [19/100], Loss training: 0.59, Accuracy training: 0.92\n","Epoch [19/100], Loss validation: 0.54, Accuracy validation: 0.92\n","Epoch [20/100], Loss training: 0.59, Accuracy training: 0.92\n","Epoch [20/100], Loss validation: 0.54, Accuracy validation: 0.92\n","Epoch [21/100], Loss training: 0.58, Accuracy training: 0.92\n","Epoch [21/100], Loss validation: 0.53, Accuracy validation: 0.92\n","Epoch [22/100], Loss training: 0.58, Accuracy training: 0.92\n","Epoch [22/100], Loss validation: 0.52, Accuracy validation: 0.92\n","Epoch [23/100], Loss training: 0.57, Accuracy training: 0.92\n","Epoch [23/100], Loss validation: 0.52, Accuracy validation: 0.92\n","Epoch [24/100], Loss training: 0.56, Accuracy training: 0.92\n","Epoch [24/100], Loss validation: 0.51, Accuracy validation: 0.92\n","Epoch [25/100], Loss training: 0.56, Accuracy training: 0.92\n","Epoch [25/100], Loss validation: 0.50, Accuracy validation: 0.92\n","Epoch [26/100], Loss training: 0.55, Accuracy training: 0.92\n","Epoch [26/100], Loss validation: 0.50, Accuracy validation: 0.92\n","Epoch [27/100], Loss training: 0.54, Accuracy training: 0.92\n","Epoch [27/100], Loss validation: 0.49, Accuracy validation: 0.92\n","Epoch [28/100], Loss training: 0.54, Accuracy training: 0.92\n","Epoch [28/100], Loss validation: 0.49, Accuracy validation: 0.92\n","Epoch [29/100], Loss training: 0.53, Accuracy training: 0.92\n","Epoch [29/100], Loss validation: 0.48, Accuracy validation: 0.92\n","Epoch [30/100], Loss training: 0.53, Accuracy training: 0.92\n","Epoch [30/100], Loss validation: 0.48, Accuracy validation: 0.92\n","Epoch [31/100], Loss training: 0.52, Accuracy training: 0.93\n","Epoch [31/100], Loss validation: 0.47, Accuracy validation: 0.92\n","Epoch [32/100], Loss training: 0.52, Accuracy training: 0.94\n","Epoch [32/100], Loss validation: 0.47, Accuracy validation: 0.92\n","Epoch [33/100], Loss training: 0.51, Accuracy training: 0.94\n","Epoch [33/100], Loss validation: 0.46, Accuracy validation: 0.92\n","Epoch [34/100], Loss training: 0.51, Accuracy training: 0.94\n","Epoch [34/100], Loss validation: 0.46, Accuracy validation: 0.92\n","Epoch [35/100], Loss training: 0.50, Accuracy training: 0.94\n","Epoch [35/100], Loss validation: 0.45, Accuracy validation: 0.92\n","Epoch [36/100], Loss training: 0.50, Accuracy training: 0.94\n","Epoch [36/100], Loss validation: 0.45, Accuracy validation: 0.92\n","Epoch [37/100], Loss training: 0.49, Accuracy training: 0.94\n","Epoch [37/100], Loss validation: 0.44, Accuracy validation: 0.92\n","Epoch [38/100], Loss training: 0.49, Accuracy training: 0.94\n","Epoch [38/100], Loss validation: 0.44, Accuracy validation: 0.92\n","Epoch [39/100], Loss training: 0.48, Accuracy training: 0.94\n","Epoch [39/100], Loss validation: 0.43, Accuracy validation: 0.92\n","Epoch [40/100], Loss training: 0.48, Accuracy training: 0.94\n","Epoch [40/100], Loss validation: 0.43, Accuracy validation: 0.92\n","Epoch [41/100], Loss training: 0.47, Accuracy training: 0.94\n","Epoch [41/100], Loss validation: 0.43, Accuracy validation: 0.92\n","Epoch [42/100], Loss training: 0.47, Accuracy training: 0.94\n","Epoch [42/100], Loss validation: 0.42, Accuracy validation: 0.92\n","Epoch [43/100], Loss training: 0.46, Accuracy training: 0.95\n","Epoch [43/100], Loss validation: 0.42, Accuracy validation: 0.92\n","Epoch [44/100], Loss training: 0.46, Accuracy training: 0.95\n","Epoch [44/100], Loss validation: 0.41, Accuracy validation: 0.92\n","Epoch [45/100], Loss training: 0.45, Accuracy training: 0.95\n","Epoch [45/100], Loss validation: 0.41, Accuracy validation: 0.92\n","Epoch [46/100], Loss training: 0.45, Accuracy training: 0.95\n","Epoch [46/100], Loss validation: 0.41, Accuracy validation: 0.92\n","Epoch [47/100], Loss training: 0.44, Accuracy training: 0.96\n","Epoch [47/100], Loss validation: 0.40, Accuracy validation: 0.92\n","Epoch [48/100], Loss training: 0.44, Accuracy training: 0.96\n","Epoch [48/100], Loss validation: 0.40, Accuracy validation: 0.92\n","Epoch [49/100], Loss training: 0.44, Accuracy training: 0.96\n","Epoch [49/100], Loss validation: 0.40, Accuracy validation: 0.92\n","Epoch [50/100], Loss training: 0.43, Accuracy training: 0.96\n","Epoch [50/100], Loss validation: 0.39, Accuracy validation: 0.92\n","Epoch [51/100], Loss training: 0.43, Accuracy training: 0.96\n","Epoch [51/100], Loss validation: 0.39, Accuracy validation: 0.92\n","Epoch [52/100], Loss training: 0.42, Accuracy training: 0.96\n","Epoch [52/100], Loss validation: 0.39, Accuracy validation: 0.92\n","Epoch [53/100], Loss training: 0.42, Accuracy training: 0.97\n","Epoch [53/100], Loss validation: 0.39, Accuracy validation: 0.92\n","Epoch [54/100], Loss training: 0.41, Accuracy training: 0.97\n","Epoch [54/100], Loss validation: 0.38, Accuracy validation: 0.92\n","Epoch [55/100], Loss training: 0.41, Accuracy training: 0.97\n","Epoch [55/100], Loss validation: 0.38, Accuracy validation: 0.92\n","Epoch [56/100], Loss training: 0.41, Accuracy training: 0.97\n","Epoch [56/100], Loss validation: 0.37, Accuracy validation: 0.92\n","Epoch [57/100], Loss training: 0.40, Accuracy training: 0.97\n","Epoch [57/100], Loss validation: 0.37, Accuracy validation: 0.92\n","Epoch [58/100], Loss training: 0.40, Accuracy training: 0.97\n","Epoch [58/100], Loss validation: 0.37, Accuracy validation: 0.92\n","Epoch [59/100], Loss training: 0.39, Accuracy training: 0.97\n","Epoch [59/100], Loss validation: 0.37, Accuracy validation: 0.92\n","Epoch [60/100], Loss training: 0.39, Accuracy training: 0.98\n","Epoch [60/100], Loss validation: 0.36, Accuracy validation: 0.92\n","Epoch [61/100], Loss training: 0.39, Accuracy training: 0.98\n","Epoch [61/100], Loss validation: 0.36, Accuracy validation: 0.92\n","Epoch [62/100], Loss training: 0.38, Accuracy training: 0.98\n","Epoch [62/100], Loss validation: 0.36, Accuracy validation: 0.96\n","Epoch [63/100], Loss training: 0.38, Accuracy training: 0.98\n","Epoch [63/100], Loss validation: 0.36, Accuracy validation: 0.96\n","Epoch [64/100], Loss training: 0.38, Accuracy training: 0.98\n","Epoch [64/100], Loss validation: 0.35, Accuracy validation: 0.96\n","Epoch [65/100], Loss training: 0.37, Accuracy training: 0.98\n","Epoch [65/100], Loss validation: 0.35, Accuracy validation: 0.96\n","Epoch [66/100], Loss training: 0.37, Accuracy training: 0.98\n","Epoch [66/100], Loss validation: 0.35, Accuracy validation: 0.96\n","Epoch [67/100], Loss training: 0.36, Accuracy training: 0.98\n","Epoch [67/100], Loss validation: 0.34, Accuracy validation: 0.96\n","Epoch [68/100], Loss training: 0.36, Accuracy training: 0.98\n","Epoch [68/100], Loss validation: 0.34, Accuracy validation: 0.96\n","Epoch [69/100], Loss training: 0.36, Accuracy training: 0.98\n","Epoch [69/100], Loss validation: 0.34, Accuracy validation: 0.96\n","Epoch [70/100], Loss training: 0.35, Accuracy training: 0.98\n","Epoch [70/100], Loss validation: 0.34, Accuracy validation: 0.96\n","Epoch [71/100], Loss training: 0.35, Accuracy training: 0.98\n","Epoch [71/100], Loss validation: 0.33, Accuracy validation: 0.96\n","Epoch [72/100], Loss training: 0.35, Accuracy training: 0.98\n","Epoch [72/100], Loss validation: 0.33, Accuracy validation: 0.96\n","Epoch [73/100], Loss training: 0.34, Accuracy training: 0.98\n","Epoch [73/100], Loss validation: 0.33, Accuracy validation: 0.96\n","Epoch [74/100], Loss training: 0.34, Accuracy training: 0.98\n","Epoch [74/100], Loss validation: 0.33, Accuracy validation: 0.96\n","Epoch [75/100], Loss training: 0.33, Accuracy training: 0.99\n","Epoch [75/100], Loss validation: 0.33, Accuracy validation: 0.96\n","Epoch [76/100], Loss training: 0.33, Accuracy training: 0.99\n","Epoch [76/100], Loss validation: 0.32, Accuracy validation: 0.96\n","Epoch [77/100], Loss training: 0.33, Accuracy training: 0.99\n","Epoch [77/100], Loss validation: 0.32, Accuracy validation: 0.96\n","Epoch [78/100], Loss training: 0.32, Accuracy training: 0.99\n","Epoch [78/100], Loss validation: 0.32, Accuracy validation: 0.96\n","Epoch [79/100], Loss training: 0.32, Accuracy training: 0.99\n","Epoch [79/100], Loss validation: 0.32, Accuracy validation: 0.96\n","Epoch [80/100], Loss training: 0.32, Accuracy training: 0.99\n","Epoch [80/100], Loss validation: 0.31, Accuracy validation: 0.96\n","Epoch [81/100], Loss training: 0.31, Accuracy training: 0.99\n","Epoch [81/100], Loss validation: 0.31, Accuracy validation: 0.96\n","Epoch [82/100], Loss training: 0.31, Accuracy training: 0.99\n","Epoch [82/100], Loss validation: 0.31, Accuracy validation: 0.96\n","Epoch [83/100], Loss training: 0.31, Accuracy training: 0.99\n","Epoch [83/100], Loss validation: 0.31, Accuracy validation: 0.96\n","Epoch [84/100], Loss training: 0.30, Accuracy training: 0.99\n","Epoch [84/100], Loss validation: 0.30, Accuracy validation: 0.96\n","Epoch [85/100], Loss training: 0.30, Accuracy training: 0.99\n","Epoch [85/100], Loss validation: 0.30, Accuracy validation: 0.96\n","Epoch [86/100], Loss training: 0.30, Accuracy training: 0.99\n","Epoch [86/100], Loss validation: 0.30, Accuracy validation: 0.96\n","Epoch [87/100], Loss training: 0.29, Accuracy training: 0.99\n","Epoch [87/100], Loss validation: 0.30, Accuracy validation: 0.96\n","Epoch [88/100], Loss training: 0.29, Accuracy training: 0.99\n","Epoch [88/100], Loss validation: 0.30, Accuracy validation: 0.96\n","Epoch [89/100], Loss training: 0.29, Accuracy training: 0.99\n","Epoch [89/100], Loss validation: 0.29, Accuracy validation: 0.96\n","Epoch [90/100], Loss training: 0.28, Accuracy training: 0.99\n","Epoch [90/100], Loss validation: 0.29, Accuracy validation: 0.96\n","Epoch [91/100], Loss training: 0.28, Accuracy training: 0.99\n","Epoch [91/100], Loss validation: 0.29, Accuracy validation: 0.96\n","Epoch [92/100], Loss training: 0.28, Accuracy training: 0.99\n","Epoch [92/100], Loss validation: 0.29, Accuracy validation: 0.96\n","Epoch [93/100], Loss training: 0.27, Accuracy training: 0.99\n","Epoch [93/100], Loss validation: 0.28, Accuracy validation: 0.96\n","Epoch [94/100], Loss training: 0.27, Accuracy training: 0.99\n","Epoch [94/100], Loss validation: 0.28, Accuracy validation: 0.96\n","Epoch [95/100], Loss training: 0.27, Accuracy training: 0.99\n","Epoch [95/100], Loss validation: 0.28, Accuracy validation: 0.96\n","Epoch [96/100], Loss training: 0.26, Accuracy training: 0.99\n","Epoch [96/100], Loss validation: 0.28, Accuracy validation: 0.96\n","Epoch [97/100], Loss training: 0.26, Accuracy training: 0.99\n","Epoch [97/100], Loss validation: 0.28, Accuracy validation: 0.96\n","Epoch [98/100], Loss training: 0.26, Accuracy training: 0.99\n","Epoch [98/100], Loss validation: 0.28, Accuracy validation: 0.96\n","Epoch [99/100], Loss training: 0.25, Accuracy training: 0.99\n","Epoch [99/100], Loss validation: 0.27, Accuracy validation: 0.96\n","Epoch [100/100], Loss training: 0.25, Accuracy training: 0.99\n","Epoch [100/100], Loss validation: 0.27, Accuracy validation: 0.96\n"]}]},{"cell_type":"code","source":["model.eval()\n","with torch.no_grad():\n","    test_output = model(X_te_tensor)\n","    _, predicted_test = torch.max(test_output, 1)\n","    test_accuracy = accuracy_score(y_te_tensor, predicted_test)\n","    print(f\"\\nFinal Test Set Accuracy: {test_accuracy:.2f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q2qvjfZG8uxy","executionInfo":{"status":"ok","timestamp":1732617940857,"user_tz":-60,"elapsed":5,"user":{"displayName":"helena liz","userId":"14461101084112642621"}},"outputId":"35d28500-c564-4f81-9709-0eeb3dbed75a"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Final Test Set Accuracy: 0.93\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"hAbeTmY39egT"},"execution_count":null,"outputs":[]}]}